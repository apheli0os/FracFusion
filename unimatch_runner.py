#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import os

# å…‹éš† GitHub ä»“åº“
get_ipython().system('git clone https://github.com/LiheYoung/UniMatch.git')

# åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•
# è¿™å¯¹äºè„šæœ¬æ­£ç¡®æ‰¾åˆ°é…ç½®æ–‡ä»¶å’Œæ¨¡å—éå¸¸é‡è¦
project_dir = '/kaggle/working/UniMatch'
os.chdir(project_dir)

print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")


# In[ ]:


# å®‰è£…é¡¹ç›®æ‰€éœ€çš„ä¾èµ–åŒ…
# -r requirements.txt ä¼šè‡ªåŠ¨å®‰è£…æ–‡ä»¶é‡Œåˆ—å‡ºçš„æ‰€æœ‰åº“
get_ipython().system('pip install -r requirements.txt')




# In[ ]:


# æ–°å¢ Cell: å¤åˆ¶é¢„è®­ç»ƒæ¨¡å‹åˆ°é¡¹ç›®ç›®å½•

import os
import shutil

# ç¡®ä¿åœ¨ UniMatch é¡¹ç›®ç›®å½•ä¸­
project_dir = '/kaggle/working/UniMatch'
os.chdir(project_dir)

# åˆ›å»ºé¡¹ç›®çš„ pretrained ç›®å½•
pretrained_dir = 'pretrained'
os.makedirs(pretrained_dir, exist_ok=True)

# æºæ–‡ä»¶è·¯å¾„ï¼ˆæ‚¨ä¸Šä¼ çš„æ¨¡å‹ï¼‰
source_model_path = '/kaggle/input/pretrained/resnet101.pth'

# ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆé¡¹ç›®æœŸæœ›çš„ä½ç½®ï¼‰
target_model_path = os.path.join(pretrained_dir, 'resnet101.pth')

# æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
if os.path.exists(source_model_path):
    print(f"âœ… æ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹: {source_model_path}")

    # å¤åˆ¶æ–‡ä»¶åˆ°é¡¹ç›®ç›®å½•
    shutil.copy2(source_model_path, target_model_path)

    print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹å·²å¤åˆ¶åˆ°: {target_model_path}")
    print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(target_model_path) / (1024*1024):.1f} MB")

    # éªŒè¯å¤åˆ¶æ˜¯å¦æˆåŠŸ
    if os.path.exists(target_model_path):
        print("âœ… é¢„è®­ç»ƒæ¨¡å‹å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
    else:
        print("âŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥ï¼")
else:
    print(f"âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶: {source_model_path}")
    print("è¯·ç¡®è®¤æ‚¨çš„ Kaggle æ•°æ®é›†åç§°å’Œæ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")

# æ˜¾ç¤º pretrained ç›®å½•çš„å†…å®¹
print(f"\npretrained ç›®å½•å†…å®¹:")
if os.path.exists(pretrained_dir):
    for file in os.listdir(pretrained_dir):
        file_path = os.path.join(pretrained_dir, file)
        file_size = os.path.getsize(file_path) / (1024*1024)
        print(f"  - {file} ({file_size:.1f} MB)")
else:
    print("  (ç›®å½•ä¸å­˜åœ¨)")


# In[ ]:


# æ–°å¢ Cell: ä¼˜åŒ–è®­ç»ƒé…ç½®ä»¥æé«˜é€Ÿåº¦

import yaml
import os

# ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
os.chdir('/kaggle/working/UniMatch')

# è¯»å–é…ç½®æ–‡ä»¶
config_file_path = 'configs/pascal.yaml'
with open(config_file_path, 'r') as f:
    config_data = yaml.safe_load(f)

print("=== å½“å‰è®­ç»ƒé…ç½® ===")
print(f"æ‰¹æ¬¡å¤§å° (batch_size): {config_data.get('batch_size', 'æœªè®¾ç½®')}")
print(f"å­¦ä¹ ç‡ (lr): {config_data.get('lr', 'æœªè®¾ç½®')}")
print(f"è£å‰ªå°ºå¯¸ (crop_size): {config_data.get('crop_size', 'æœªè®¾ç½®')}")

# === ä¼˜åŒ–é…ç½® ===
print("\n=== ä¼˜åŒ–é…ç½® ===")

# 1. å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆåŸæ¥å¯èƒ½æ˜¯2ï¼Œæˆ‘ä»¬å¯ä»¥å¢åŠ åˆ°6-8ï¼‰
original_batch_size = config_data.get('batch_size', 2)
optimized_batch_size = 8  # æ ¹æ®15GBæ˜¾å­˜ï¼Œå¯ä»¥å°è¯•8
config_data['batch_size'] = optimized_batch_size
config_data['backbone'] = 'resnet101'
# 2. ç›¸åº”è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæ‰¹æ¬¡å¤§å°å¢åŠ æ—¶ï¼Œé€šå¸¸éœ€è¦çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼‰
original_lr = config_data.get('lr', 0.001)
lr_scale_factor = optimized_batch_size / original_batch_size
optimized_lr = original_lr * lr_scale_factor
config_data['lr'] = optimized_lr

# 3. å¯é€‰ï¼šå¢åŠ å›¾åƒå°ºå¯¸ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
# config_data['crop_size'] = 513  # ä»321å¢åŠ åˆ°513ï¼Œä½†è¿™ä¼šå¢åŠ æ˜¾å­˜ä½¿ç”¨

print(f"æ‰¹æ¬¡å¤§å°: {original_batch_size} â†’ {optimized_batch_size}")
print(f"å­¦ä¹ ç‡: {original_lr} â†’ {optimized_lr:.6f}")
print(f"å­¦ä¹ ç‡ç¼©æ”¾å› å­: {lr_scale_factor}")
#print(f"å­¦ä¹ ç‡ç¼©æ”¾å› å­: {lr_scale_factor}")

# ä¿å­˜ä¼˜åŒ–åçš„é…ç½®
with open(config_file_path, 'w') as f:
    yaml.dump(config_data, f)

print("âœ… é…ç½®ä¼˜åŒ–å®Œæˆï¼")


# In[ ]:


# ä¿®å¤ Cell: æ­£ç¡®è®¾ç½®é…ç½®å‚æ•°

import yaml
import os

# ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
os.chdir('/kaggle/working/UniMatch')

# è¯»å–é…ç½®æ–‡ä»¶
config_file_path = 'configs/pascal.yaml'
with open(config_file_path, 'r') as f:
    config_data = yaml.safe_load(f)

print("=== å½“å‰è®­ç»ƒé…ç½®ï¼ˆä¿®å¤å‰ï¼‰===")
print(f"æ‰¹æ¬¡å¤§å° (batch_size): {config_data.get('batch_size', 'æœªè®¾ç½®')} (ç±»å‹: {type(config_data.get('batch_size', 'æœªè®¾ç½®'))})")
print(f"éª¨å¹²ç½‘ç»œ (backbone): {config_data.get('backbone', 'æœªè®¾ç½®')}")
print(f"å­¦ä¹ ç‡ (lr): {config_data.get('lr', 'æœªè®¾ç½®')}")
print(f"è£å‰ªå°ºå¯¸ (crop_size): {config_data.get('crop_size', 'æœªè®¾ç½®')}")

# === ä¿®å¤å’Œä¼˜åŒ–é…ç½® ===
print("\n=== ä¿®å¤å’Œä¼˜åŒ–é…ç½® ===")

# 1. æ­£ç¡®è®¾ç½®æ‰¹æ¬¡å¤§å°ï¼ˆä¿®å¤ä¹‹å‰çš„é”™è¯¯ï¼‰
config_data['batch_size'] = 6  # è®¾ç½®ä¸ºæ­£ç¡®çš„æ•°å­—ç±»å‹

# 2. æ­£ç¡®è®¾ç½®éª¨å¹²ç½‘ç»œ
config_data['backbone'] = 'resnet101'

# 3. é‡æ–°è®¡ç®—å­¦ä¹ ç‡
original_batch_size = 2  # åŸå§‹çš„åŸºç¡€æ‰¹æ¬¡å¤§å°
current_batch_size = 8   # å½“å‰è®¾ç½®çš„æ‰¹æ¬¡å¤§å°
base_lr = 0.001          # åŸºç¡€å­¦ä¹ ç‡

# æ ¹æ®æ‰¹æ¬¡å¤§å°çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡
lr_scale_factor = current_batch_size / original_batch_size
optimized_lr = base_lr * lr_scale_factor
config_data['lr'] = optimized_lr

print(f"æ‰¹æ¬¡å¤§å°: ä¿®å¤ä¸º {config_data['batch_size']} (æ•°å­—ç±»å‹)")
print(f"éª¨å¹²ç½‘ç»œ: {config_data['backbone']}")
print(f"å­¦ä¹ ç‡: {base_lr} â†’ {optimized_lr:.6f}")
print(f"å­¦ä¹ ç‡ç¼©æ”¾å› å­: {lr_scale_factor}")

# 4. ç¡®ä¿å…¶ä»–é‡è¦å‚æ•°æ­£ç¡®
config_data['crop_size'] = 321  # ç¡®ä¿crop_sizeæ˜¯æ•°å­—
config_data['nclass'] = 21      # PASCAL VOCçš„ç±»åˆ«æ•°

# ä¿å­˜ä¼˜åŒ–åçš„é…ç½®
with open(config_file_path, 'w') as f:
    yaml.dump(config_data, f)

print("\n=== ä¿®å¤åçš„é…ç½® ===")
print(f"æ‰¹æ¬¡å¤§å° (batch_size): {config_data['batch_size']} (ç±»å‹: {type(config_data['batch_size'])})")
print(f"éª¨å¹²ç½‘ç»œ (backbone): {config_data['backbone']}")
print(f"å­¦ä¹ ç‡ (lr): {config_data['lr']}")
print(f"è£å‰ªå°ºå¯¸ (crop_size): {config_data['crop_size']}")

print("âœ… é…ç½®ä¿®å¤å’Œä¼˜åŒ–å®Œæˆï¼")


# In[ ]:


# æ–°å¢ Cell: æ•°æ®åŠ è½½å’Œè®­ç»ƒä¼˜åŒ–

import yaml
import os

os.chdir('/kaggle/working/UniMatch')

config_file_path = 'configs/pascal.yaml'
with open(config_file_path, 'r') as f:
    config_data = yaml.safe_load(f)

# æ•°æ®åŠ è½½ä¼˜åŒ–
data_loading_optimizations = {
    'num_workers': 6,  # å¢åŠ æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹
    'pin_memory': True,  # å¯ç”¨å›ºå®šå†…å­˜
    'persistent_workers': True,  # ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
    'prefetch_factor': 4,  # é¢„å–å› å­
}

print("=== æ•°æ®åŠ è½½ä¼˜åŒ– ===")
for key, value in data_loading_optimizations.items():
    old_value = config_data.get(key, 'æœªè®¾ç½®')
    config_data[key] = value
    print(f"{key}: {old_value} â†’ {value}")

# ä¿å­˜é…ç½®
with open(config_file_path, 'w') as f:
    yaml.dump(config_data, f)

print("âœ… æ•°æ®åŠ è½½ä¼˜åŒ–å·²åº”ç”¨")


# In[ ]:


# æ–°å¢ Cell: è°ƒæ•´éªŒè¯é¢‘ç‡

import yaml
import os

os.chdir('/kaggle/working/UniMatch')

config_file_path = 'configs/pascal.yaml'
with open(config_file_path, 'r') as f:
    config_data = yaml.safe_load(f)

# è°ƒæ•´éªŒè¯é¢‘ç‡
# åŸæ¥å¯èƒ½æ¯ä¸ªepochéƒ½éªŒè¯ï¼Œç°åœ¨æ”¹ä¸ºæ¯2-3ä¸ªepochéªŒè¯ä¸€æ¬¡
config_data['eval_freq'] = 5  # æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡
config_data['epochs'] = 80  # å‡å°‘æ€»è½®æ•°
print(f"éªŒè¯é¢‘ç‡: æ¯ä¸ªepoch â†’ æ¯{config_data['eval_freq']}ä¸ªepoch")
print("é¢„æœŸåŠ é€Ÿ: å‡å°‘éªŒè¯æ—¶é—´çº¦50%")

# ä¿å­˜é…ç½®
with open(config_file_path, 'w') as f:
    yaml.dump(config_data, f)

print("âœ… éªŒè¯é¢‘ç‡å·²è°ƒæ•´")


# In[ ]:


# æœ€ç»ˆä¿®å¤è„šæœ¬ - ç¡®ä¿æ‰€æœ‰torch.loadéƒ½æœ‰weights_only=False

import os
import re

os.chdir('/kaggle/working/UniMatch')

def comprehensive_torch_load_fix():
    """å…¨é¢ä¿®å¤torch.loadå…¼å®¹æ€§é—®é¢˜"""

    # é¦–å…ˆåˆ é™¤æœ‰é—®é¢˜çš„checkpointæ–‡ä»¶ï¼Œé‡æ–°å¼€å§‹è®­ç»ƒ
    problematic_checkpoint = '/kaggle/working/exp/pascal/unimatch/r101/732/latest.pth'
    if os.path.exists(problematic_checkpoint):
        os.remove(problematic_checkpoint)
        print(f"âœ… å·²åˆ é™¤æœ‰é—®é¢˜çš„checkpoint: {problematic_checkpoint}")

    # ä¿®å¤æ‰€æœ‰Pythonæ–‡ä»¶ä¸­çš„torch.load
    python_files = []
    for root, dirs, files in os.walk('.'):
        for file in files:
            if file.endswith('.py'):
                python_files.append(os.path.join(root, file))

    print(f"æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶")

    fixed_files = []
    for file_path in python_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            original_content = content

            # ä¿®å¤torch.loadè°ƒç”¨ - æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
            # 1. æ²¡æœ‰weights_onlyå‚æ•°çš„torch.load
            pattern1 = r'torch\.load\(([^)]*)\)(?![^,]*weights_only)'

            def replace_torch_load(match):
                args = match.group(1).strip()
                if not args:
                    return 'torch.load(weights_only=False)'
                elif args.endswith(','):
                    return f'torch.load({args} weights_only=False)'
                else:
                    return f'torch.load({args}, weights_only=False)'

            content = re.sub(pattern1, replace_torch_load, content)

            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                fixed_files.append(file_path)

        except Exception as e:
            print(f"âš ï¸  æ— æ³•å¤„ç†æ–‡ä»¶ {file_path}: {e}")

    print(f"âœ… å·²ä¿®å¤ {len(fixed_files)} ä¸ªæ–‡ä»¶:")
    for file_path in fixed_files:
        print(f"  - {file_path}")

    return fixed_files

# è¿è¡Œä¿®å¤
fixed_files = comprehensive_torch_load_fix()

# ç‰¹åˆ«æ£€æŸ¥å…³é”®æ–‡ä»¶
key_files = ['unimatch.py', 'train_unimatch_ftv.py']
print("\n=== å…³é”®æ–‡ä»¶æ£€æŸ¥ ===")

for file_path in key_files:
    if os.path.exists(file_path):
        print(f"\n{file_path}:")
        with open(file_path, 'r') as f:
            lines = f.readlines()

        torch_load_lines = []
        for i, line in enumerate(lines):
            if 'torch.load(' in line:
                torch_load_lines.append((i+1, line.strip()))

        if torch_load_lines:
            for line_num, line_content in torch_load_lines:
                print(f"  ç¬¬{line_num}è¡Œ: {line_content}")
        else:
            print("  æœªæ‰¾åˆ°torch.loadè°ƒç”¨")

print("\nğŸ‰ PyTorch 2.6å…¼å®¹æ€§ä¿®å¤å®Œæˆï¼")
print("ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œæ‚¨çš„è®­ç»ƒå‘½ä»¤ã€‚")


# In[ ]:


# Cell å¼•å…¥åˆ†æ•°é˜¶å¢å¼º
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import gamma
import cv2
from PIL import Image
import requests
from io import BytesIO
import os


plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå· '-' æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
# ä½¿ç”¨æ‚¨å·²æœ‰çš„åˆ†æ•°é˜¶ç³»æ•°è®¡ç®—æ¨¡å—
class FractionalCoefficients:
    """åŸºäºæ‚¨æä¾›å…¬å¼(18)çš„åˆ†æ•°é˜¶ç³»æ•°è®¡ç®—ç±»"""

    @staticmethod
    def compute_coefficients(v, max_order=10):
        """è®¡ç®—åˆ†æ•°é˜¶ç³»æ•° C_k"""
        coeffs = {}

        # åŸºäºæ‚¨æä¾›çš„å…¬å¼è®¡ç®—
        coeffs[-1] = v/4 + (v**2)/8                    # C_{-1}
        coeffs[0] = 1 - (v**2)/8 - (v**3)/8            # C_0  
        coeffs[1] = -5*v/4 + 5*(v**2)/16 + (v**4)/16   # C_1

        # é€šç”¨å…¬å¼è®¡ç®—æ›´é«˜é˜¶é¡¹
        for k in range(2, max_order + 1):
            try:
                c_k = (gamma(k-v+1) / (gamma(k+1) * gamma(-v))) * (v/4 + v**2/8)
                c_k += (gamma(k-v) / gamma(k+1)) * (1 - v**2/4)
                c_k += (gamma(k-v-1) / (gamma(k-1) * gamma(-v))) * (-v/4 + v**2/8)
                coeffs[k] = c_k
            except:
                # Gammaå‡½æ•°è®¡ç®—å¤±è´¥æ—¶ä½¿ç”¨é€’æ¨å…³ç³»
                coeffs[k] = coeffs[k-1] * (k-v) / k

        return coeffs

class FractionalMaskGenerator:
    """8æ–¹å‘åˆ†æ•°é˜¶æ©è†œç”Ÿæˆå™¨"""

    def __init__(self, v=0.5, mask_size=7):
        self.v = v
        self.mask_size = mask_size
        self.coeffs = FractionalCoefficients.compute_coefficients(v, mask_size//2)

    def generate_8_direction_masks(self):
        """ç”Ÿæˆ8ä¸ªæ–¹å‘çš„åˆ†æ•°é˜¶æ©è†œ"""
        masks = {}
        center = self.mask_size // 2

        # æ°´å¹³æ–¹å‘
        mask_h = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                mask_h[center, center + i] = coeff
        masks['horizontal'] = mask_h

        # å‚ç›´æ–¹å‘
        mask_v = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                mask_v[center + i, center] = coeff
        masks['vertical'] = mask_v

        # ä¸»å¯¹è§’çº¿æ–¹å‘
        mask_d1 = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                if 0 <= center + i < self.mask_size and 0 <= center + i < self.mask_size:
                    mask_d1[center + i, center + i] = coeff
        masks['diagonal_main'] = mask_d1

        # åå¯¹è§’çº¿æ–¹å‘
        mask_d2 = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                if 0 <= center + i < self.mask_size and 0 <= center - i < self.mask_size:
                    mask_d2[center + i, center - i] = coeff
        masks['diagonal_anti'] = mask_d2

        # é¢å¤–çš„4ä¸ªæ–¹å‘ç”¨äº8æ–¹å‘å¢å¼º
        # ä¸œåŒ—æ–¹å‘ (45åº¦)
        mask_ne = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                row, col = center - i, center + i
                if 0 <= row < self.mask_size and 0 <= col < self.mask_size:
                    mask_ne[row, col] = coeff
        masks['northeast'] = mask_ne

        # è¥¿åŒ—æ–¹å‘ (135åº¦)
        mask_nw = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                row, col = center - i, center - i
                if 0 <= row < self.mask_size and 0 <= col < self.mask_size:
                    mask_nw[row, col] = coeff
        masks['northwest'] = mask_nw

        # ä¸œå—æ–¹å‘ (-45åº¦)
        mask_se = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                row, col = center + i, center + i
                if 0 <= row < self.mask_size and 0 <= col < self.mask_size:
                    mask_se[row, col] = coeff
        masks['southeast'] = mask_se

        # è¥¿å—æ–¹å‘ (-135åº¦)
        mask_sw = torch.zeros(self.mask_size, self.mask_size)
        for i, coeff in self.coeffs.items():
            if abs(i) <= center:
                row, col = center + i, center - i
                if 0 <= row < self.mask_size and 0 <= col < self.mask_size:
                    mask_sw[row, col] = coeff
        masks['southwest'] = mask_sw

        return masks

class FractionalImageEnhancer:
    """
    æ¨¡å—åŒ–åˆ†æ•°é˜¶å›¾åƒå¢å¼ºç±»

    æ­¤ç±»æä¾›äº†ä½¿ç”¨8æ–¹å‘å·ç§¯æ©è†œè¿›è¡Œåˆ†æ•°é˜¶å›¾åƒå¢å¼ºçš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

    å‚æ•°:
        fractional_order (float): åˆ†æ•°é˜¶å‚æ•° v (0.1 åˆ° 2.0)
        mask_size (int): å·ç§¯æ©è†œå¤§å° (é»˜è®¤: 7)
        enhancement_factor (float): å¢å¼ºå¼ºåº¦ (é»˜è®¤: 0.3)
        enhancement_mode (str): å¢å¼ºç­–ç•¥ ('average', 'weighted', 'selective')
    """

    def __init__(self, fractional_order=0.5, mask_size=7, enhancement_factor=0.3, enhancement_mode='weighted'):
        self.fractional_order = fractional_order
        self.mask_size = mask_size
        self.enhancement_factor = enhancement_factor
        self.enhancement_mode = enhancement_mode

        # ç”Ÿæˆåˆ†æ•°é˜¶æ©è†œ
        self.mask_generator = FractionalMaskGenerator(fractional_order, mask_size)
        self.masks = self.mask_generator.generate_8_direction_masks()

        # åŠ æƒå¢å¼ºçš„æ–¹å‘æƒé‡
        self.direction_weights = {
            'horizontal': 1.0,
            'vertical': 1.0,
            'diagonal_main': 0.8,
            'diagonal_anti': 0.8,
            'northeast': 0.6,
            'northwest': 0.6,
            'southeast': 0.6,
            'southwest': 0.6
        }

    def _preprocess_image(self, image):
        """å°†è¾“å…¥å›¾åƒé¢„å¤„ç†ä¸ºæ ‡å‡†æ ¼å¼"""
        # å¤„ç†ä¸åŒçš„è¾“å…¥æ ¼å¼
        if isinstance(image, np.ndarray):
            if image.dtype == np.uint8:
                image = image.astype(np.float32) / 255.0
            image = torch.from_numpy(image)

        if isinstance(image, Image.Image):
            image = torch.tensor(np.array(image), dtype=torch.float32) / 255.0

        # ç¡®ä¿æ­£ç¡®çš„å¼ é‡æ ¼å¼ [C, H, W]
        if len(image.shape) == 3:
            if image.shape[2] == 3:  # HWC æ ¼å¼
                image = image.permute(2, 0, 1)  # è½¬æ¢ä¸º CHW
        elif len(image.shape) == 2:  # ç°åº¦å›¾
            image = image.unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦

        return image

    def _extract_directional_features(self, image):
        """æå–æ‰€æœ‰8ä¸ªæ–¹å‘çš„çº¹ç†ç‰¹å¾"""
        # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œå¤„ç†
        if image.shape[0] == 3:  # RGB
            gray_image = torch.mean(image, dim=0, keepdim=True)
        else:
            gray_image = image

        # ä¸ºå·ç§¯æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        gray_image = gray_image.unsqueeze(0)  # [1, 1, H, W]

        directional_features = {}

        # åº”ç”¨æ¯ä¸ªæ©è†œ
        for direction, mask in self.masks.items():
            conv_kernel = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, mask_size, mask_size]
            padding = self.mask_size // 2

            # ä¸åˆ†æ•°é˜¶æ©è†œå·ç§¯
            feature = F.conv2d(gray_image, conv_kernel, padding=padding)
            directional_features[direction] = feature.squeeze()

        return directional_features

    def _combine_features(self, directional_features):
        """æ ¹æ®å¢å¼ºæ¨¡å¼ç»„åˆæ–¹å‘ç‰¹å¾"""
        feature_list = list(directional_features.values())

        if self.enhancement_mode == 'average':
            # ç®€å•å¹³å‡æ‰€æœ‰æ–¹å‘
            combined = torch.stack(feature_list).mean(dim=0)

        elif self.enhancement_mode == 'weighted':
            # åŸºäºæ–¹å‘é‡è¦æ€§çš„åŠ æƒç»„åˆ
            combined = torch.zeros_like(feature_list[0])
            total_weight = 0

            for direction, feature in directional_features.items():
                weight = self.direction_weights.get(direction, 1.0)
                combined += weight * feature
                total_weight += weight

            combined = combined / total_weight

        elif self.enhancement_mode == 'selective':
            # è‡ªé€‚åº”é€‰æ‹©æœ€å¼ºå“åº”
            feature_stack = torch.stack(feature_list)  # [8, H, W]

            # è®¡ç®—æ¯ä¸ªæ–¹å‘çš„å±€éƒ¨æ–¹å·®
            variances = torch.var(feature_stack, dim=(1, 2))

            # æ ¹æ®æ–¹å·®åŠ æƒï¼ˆçº¹ç†æ›´å¼ºçš„æ–¹å‘æƒé‡æ›´é«˜ï¼‰
            weights = F.softmax(variances * 10, dim=0)  # æ¸©åº¦ç¼©æ”¾

            combined = torch.zeros_like(feature_list[0])
            for i, feature in enumerate(feature_list):
                combined += weights[i] * feature

        else:
            raise ValueError(f"æœªçŸ¥çš„å¢å¼ºæ¨¡å¼: {self.enhancement_mode}")

        return combined

    def enhance_image(self, image):
        """
        ä¸»è¦å¢å¼ºå‡½æ•°

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (PIL.Image, numpy.ndarray, æˆ– torch.Tensor)
                  - å¯¹äºnumpy: å½¢çŠ¶åº”ä¸º (H, W) æˆ– (H, W, C)
                  - å¯¹äºtensor: å½¢çŠ¶åº”ä¸º (C, H, W) æˆ– (H, W)

        è¿”å›:
            enhanced_image: å¢å¼ºåçš„å›¾åƒ torch.Tensor [C, H, W]
            enhancement_map: å¢å¼ºç»†èŠ‚å›¾
            directional_features: æ–¹å‘ç‰¹å¾å­—å…¸
        """
        # é¢„å¤„ç†è¾“å…¥
        original_image = self._preprocess_image(image)
        original_shape = original_image.shape

        # æå–æ–¹å‘ç‰¹å¾
        directional_features = self._extract_directional_features(original_image)

        # ç»„åˆç‰¹å¾
        enhancement_map = self._combine_features(directional_features)

        # åº”ç”¨å¢å¼º
        if len(original_shape) == 3:  # å½©è‰²å›¾åƒ
            enhanced_image = original_image.clone()
            for c in range(original_image.shape[0]):
                enhanced_image[c] += self.enhancement_factor * enhancement_map
        else:  # ç°åº¦å›¾
            enhanced_image = original_image + self.enhancement_factor * enhancement_map

        # å°†å€¼é™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
        enhanced_image = torch.clamp(enhanced_image, 0, 1)

        return enhanced_image, enhancement_map, directional_features

    def set_parameters(self, fractional_order=None, enhancement_factor=None, enhancement_mode=None):
        """æ›´æ–°å¢å¼ºå‚æ•°"""
        if fractional_order is not None:
            self.fractional_order = fractional_order
            # ç”¨æ–°çš„é˜¶æ•°é‡æ–°ç”Ÿæˆæ©è†œ
            self.mask_generator = FractionalMaskGenerator(fractional_order, self.mask_size)
            self.masks = self.mask_generator.generate_8_direction_masks()

        if enhancement_factor is not None:
            self.enhancement_factor = enhancement_factor

        if enhancement_mode is not None:
            self.enhancement_mode = enhancement_mode


# In[ ]:


# Cell: æ•°æ®é›†åˆ†æ•°é˜¶å¢å¼ºé¢„å¤„ç†
import os
import yaml
from PIL import Image
import torch
from tqdm import tqdm
import shutil

def preprocess_dataset_with_fractional_enhancement():
    """
    å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œåˆ†æ•°é˜¶å¢å¼ºé¢„å¤„ç†
    """
    print("ğŸš€ å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œåˆ†æ•°é˜¶å¢å¼ºé¢„å¤„ç†...")

    # 1. è®¾ç½®è·¯å¾„
    original_data_root = '/kaggle/input/pascal1-3/VOC2012'
    enhanced_data_root = '/kaggle/working/enhanced_VOC2012'

    original_img_dir = os.path.join(original_data_root, 'JPEGImages')
    enhanced_img_dir = os.path.join(enhanced_data_root, 'JPEGImages')

    # 2. åˆ›å»ºå¢å¼ºåçš„æ•°æ®ç›®å½•
    os.makedirs(enhanced_img_dir, exist_ok=True)

    # 3. å¤åˆ¶å…¶ä»–å¿…è¦ç›®å½•ï¼ˆæ ‡ç­¾ç­‰ï¼‰
    for dir_name in ['SegmentationClass', 'SegmentationObject', 'ImageSets']:
        src_dir = os.path.join(original_data_root, dir_name)
        dst_dir = os.path.join(enhanced_data_root, dir_name)
        if os.path.exists(src_dir):
            if os.path.exists(dst_dir):
                shutil.rmtree(dst_dir)
            shutil.copytree(src_dir, dst_dir)
            print(f"âœ… å¤åˆ¶ç›®å½•: {dir_name}")

    # 4. åˆå§‹åŒ–åˆ†æ•°é˜¶å¢å¼ºå™¨
    enhancer = FractionalImageEnhancer(
        fractional_order=0.6,          # åˆ†æ•°é˜¶å‚æ•°
        enhancement_factor=0.2,        # é€‚ä¸­çš„å¢å¼ºå¼ºåº¦
        enhancement_mode='weighted'    # åŠ æƒæ¨¡å¼
    )

    # 5. è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    img_files = [f for f in os.listdir(original_img_dir) 
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    print(f"æ‰¾åˆ° {len(img_files)} å¼ å›¾åƒéœ€è¦å¤„ç†")

    # 6. æ‰¹é‡å¤„ç†å›¾åƒ
    processed_count = 0
    failed_count = 0

    for img_file in tqdm(img_files, desc="å¤„ç†å›¾åƒ"):
        try:
            # åŠ è½½åŸå§‹å›¾åƒ
            img_path = os.path.join(original_img_dir, img_file)
            original_img = Image.open(img_path).convert('RGB')

            # åº”ç”¨åˆ†æ•°é˜¶å¢å¼º
            enhanced_tensor, _, _ = enhancer.enhance_image(original_img)

            # è½¬æ¢å›PILå›¾åƒ
            enhanced_array = (enhanced_tensor.permute(1, 2, 0).numpy() * 255).astype('uint8')
            enhanced_img = Image.fromarray(enhanced_array)

            # ä¿å­˜å¢å¼ºåçš„å›¾åƒ
            enhanced_path = os.path.join(enhanced_img_dir, img_file)
            enhanced_img.save(enhanced_path, quality=95)

            processed_count += 1

        except Exception as e:
            print(f"âŒ å¤„ç†å›¾åƒå¤±è´¥ {img_file}: {str(e)}")
            failed_count += 1

            # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œå¤åˆ¶åŸå›¾åƒ
            try:
                shutil.copy2(img_path, os.path.join(enhanced_img_dir, img_file))
                processed_count += 1
            except:
                pass

    print(f"\nâœ… é¢„å¤„ç†å®Œæˆ:")
    print(f"   æˆåŠŸå¤„ç†: {processed_count} å¼ å›¾åƒ")
    print(f"   å¤±è´¥: {failed_count} å¼ å›¾åƒ")
    print(f"   å¢å¼ºåæ•°æ®é›†ä½ç½®: {enhanced_data_root}")

    return enhanced_data_root

def update_config_for_enhanced_data(enhanced_data_root):
    """
    æ›´æ–°é…ç½®æ–‡ä»¶ä»¥ä½¿ç”¨å¢å¼ºåçš„æ•°æ®é›†
    """
    print("\nğŸ”§ æ›´æ–°é…ç½®æ–‡ä»¶...")

    # ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
    project_dir = '/kaggle/working/UniMatch'
    os.chdir(project_dir)

    config_file_path = 'configs/pascal.yaml'
    with open(config_file_path, 'r') as f:
        config_data = yaml.safe_load(f)

    # æ›´æ–°æ•°æ®æ ¹è·¯å¾„
    print(f"åŸæ•°æ®è·¯å¾„: {config_data.get('data_root', 'æœªè®¾ç½®')}")
    config_data['data_root'] = enhanced_data_root
    print(f"æ–°æ•°æ®è·¯å¾„: {enhanced_data_root}")

    # ä¿å­˜é…ç½®
    with open(config_file_path, 'w') as f:
        yaml.dump(config_data, f)

    print("âœ… é…ç½®æ–‡ä»¶æ›´æ–°å®Œæˆ")
    return config_data

def create_enhanced_splits():
    """
    åˆ›å»ºå¢å¼ºåæ•°æ®é›†çš„splitsæ–‡ä»¶
    """
    print("\nğŸ“ åˆ›å»ºå¢å¼ºåæ•°æ®é›†çš„splits...")

    project_dir = '/kaggle/working/UniMatch'
    os.chdir(project_dir)

    # è¯»å–åŸå§‹splits
    dataset = 'pascal'
    split = '732'

    labeled_id_path = f'splits/{dataset}/{split}/labeled.txt'
    unlabeled_id_path = f'splits/{dataset}/{split}/unlabeled.txt'

    # éªŒè¯splitsæ–‡ä»¶å­˜åœ¨
    if os.path.exists(labeled_id_path) and os.path.exists(unlabeled_id_path):
        print(f"âœ… ä½¿ç”¨ç°æœ‰çš„splitsæ–‡ä»¶:")
        print(f"   æ ‡è®°æ•°æ®: {labeled_id_path}")
        print(f"   æ— æ ‡è®°æ•°æ®: {unlabeled_id_path}")

        # ç»Ÿè®¡æ•°é‡
        with open(labeled_id_path, 'r') as f:
            labeled_count = len(f.readlines())
        with open(unlabeled_id_path, 'r') as f:
            unlabeled_count = len(f.readlines())

        print(f"   æ ‡è®°æ ·æœ¬æ•°: {labeled_count}")
        print(f"   æ— æ ‡è®°æ ·æœ¬æ•°: {unlabeled_count}")

        return True
    else:
        print("âŒ splitsæ–‡ä»¶ä¸å­˜åœ¨")
        return False

# ä¸»é¢„å¤„ç†æµç¨‹
def main_preprocessing():
    """
    ä¸»è¦çš„é¢„å¤„ç†æµç¨‹
    """
    print("=" * 60)
    print("ğŸ¯ å¼€å§‹åˆ†æ•°é˜¶å¢å¼ºæ•°æ®é¢„å¤„ç†æµç¨‹")
    print("=" * 60)

    # Step 1: å¯¹æ•°æ®é›†è¿›è¡Œåˆ†æ•°é˜¶å¢å¼º
    enhanced_data_root = preprocess_dataset_with_fractional_enhancement()

    # Step 2: æ›´æ–°é…ç½®æ–‡ä»¶
    config = update_config_for_enhanced_data(enhanced_data_root)

    # Step 3: éªŒè¯splitsæ–‡ä»¶
    splits_ok = create_enhanced_splits()

    if splits_ok:
        print("\nğŸ‰ é¢„å¤„ç†æµç¨‹å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")
        print(f"å¢å¼ºåçš„æ•°æ®é›†è·¯å¾„: {enhanced_data_root}")
        return True
    else:
        print("\nâŒ é¢„å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")
        return False

# è¿è¡Œé¢„å¤„ç†
# if main_preprocessing():
#     print("\n" + "=" * 60)
#     print("ğŸš€ å‡†å¤‡å¼€å§‹ UniMatch è®­ç»ƒ...")
#     print("=" * 60)
# else:
#     print("âŒ é¢„å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")


# In[ ]:


# Cell: ä¿®å¤ torch.load è¯­æ³•é”™è¯¯

import os
import re

os.chdir('/kaggle/working/UniMatch')

def fix_torch_load_syntax_error():
    """ä¿®å¤torch.loadä¸­é‡å¤çš„weights_onlyå‚æ•°"""

    # æŸ¥æ‰¾æ‰€æœ‰Pythonæ–‡ä»¶
    python_files = []
    for root, dirs, files in os.walk('.'):
        for file in files:
            if file.endswith('.py'):
                python_files.append(os.path.join(root, file))

    print(f"æ£€æŸ¥ {len(python_files)} ä¸ªPythonæ–‡ä»¶...")

    fixed_files = []
    for file_path in python_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            original_content = content

            # ä¿®å¤é‡å¤çš„weights_onlyå‚æ•°
            # æ‰¾åˆ°æ‰€æœ‰torch.loadè°ƒç”¨
            pattern = r'torch\.load\(([^)]*)\)'

            def fix_weights_only(match):
                args_str = match.group(1)

                # ç§»é™¤é‡å¤çš„weights_onlyå‚æ•°
                # é¦–å…ˆç§»é™¤æ‰€æœ‰weights_onlyå‚æ•°
                args_str = re.sub(r',?\s*weights_only\s*=\s*False', '', args_str)
                args_str = re.sub(r'weights_only\s*=\s*False\s*,?', '', args_str)

                # æ¸…ç†å¤šä½™çš„é€—å·
                args_str = re.sub(r',\s*,', ',', args_str)
                args_str = re.sub(r'^\s*,', '', args_str)
                args_str = re.sub(r',\s*$', '', args_str)

                # æ·»åŠ ä¸€ä¸ªweights_only=Falseå‚æ•°
                if args_str.strip():
                    if not args_str.strip().endswith(','):
                        args_str += ', '
                    args_str += 'weights_only=False'
                else:
                    args_str = 'weights_only=False'

                return f'torch.load({args_str})'

            # åº”ç”¨ä¿®å¤
            content = re.sub(pattern, fix_weights_only, content)

            # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                fixed_files.append(file_path)
                print(f"âœ… ä¿®å¤æ–‡ä»¶: {file_path}")

        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    return fixed_files

# è¿è¡Œä¿®å¤
fixed_files = fix_torch_load_syntax_error()

# ç‰¹åˆ«æ£€æŸ¥unimatch.pyæ–‡ä»¶
unimatch_file = 'unimatch.py'
if os.path.exists(unimatch_file):
    print(f"\n=== æ£€æŸ¥ {unimatch_file} ===")
    with open(unimatch_file, 'r') as f:
        lines = f.readlines()

    # æŸ¥æ‰¾åŒ…å«torch.loadçš„è¡Œ
    for i, line in enumerate(lines):
        if 'torch.load(' in line:
            print(f"ç¬¬{i+1}è¡Œ: {line.strip()}")

            # æ£€æŸ¥æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯
            if line.count('weights_only') > 1:
                print(f"âŒ ç¬¬{i+1}è¡Œæœ‰é‡å¤çš„weights_onlyå‚æ•°")
            elif 'weights_only=False' in line:
                print(f"âœ… ç¬¬{i+1}è¡Œè¯­æ³•æ­£ç¡®")

print(f"\nâœ… è¯­æ³•é”™è¯¯ä¿®å¤å®Œæˆï¼Œå…±ä¿®å¤ {len(fixed_files)} ä¸ªæ–‡ä»¶")


# In[ ]:


import torch
import torch.nn as nn
import torch.nn.functional as F

class FractionalFusionUp(nn.Module):
    """
    åˆ†æ•°é˜¶è‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼šæ›¿ä»£æœ€ç»ˆ logits çš„åŒçº¿æ€§æ’å€¼
    - ä»…æ”¾å¤§ç©ºé—´å°ºå¯¸ï¼Œä¸æ”¹å˜é€šé“ï¼ˆç±»åˆ«ï¼‰æ•°
    - é¢„æµ‹8æ–¹å‘åˆ†æ•°é˜¶æ¬¡ v âˆˆ [-vmax,vmax] ä¸é—¨æ§ g âˆˆ [0,1]
    - è¾“å‡º: y [B,C,H,W], orders [B,8,h,w], gate [B,1,H,W]
    """
    def __init__(self, in_channels, up_factor=4, kernel_size=3, vmax=1.0, hidden=48,
                 mode='both', beta=1.6, tau=0.5, center_bias=2.5, smooth_residual=0.15):
        super().__init__()
        self.k = int(kernel_size)
        self.s = int(up_factor)
        self.vmax = float(vmax)
        self.mode = mode
        self.center = self.k // 2
        self.beta = nn.Parameter(torch.tensor(float(beta)))
        self.tau = float(tau)
        self.center_bias = float(center_bias)
        self.smooth_residual = float(smooth_residual)

        dirs = torch.tensor([[1,0],[1,1],[0,1],[-1,1],[-1,0],[-1,-1],[0,-1],[1,-1]], dtype=torch.float32)
        self.register_buffer('dirs', dirs / (dirs.norm(dim=1, keepdim=True) + 1e-8))

        self.order_head = nn.Sequential(
            nn.Conv2d(in_channels, hidden, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(hidden, 8, 3, 1, 1)
        )
        self.gate = nn.Sequential(
            nn.Conv2d(in_channels, hidden, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(hidden, 1, 3, 1, 1), nn.Sigmoid()
        )

        base = []
        with torch.no_grad():
            yy, xx = torch.meshgrid(torch.arange(self.k), torch.arange(self.k), indexing='ij')
            rel = torch.stack([yy - self.center, xx - self.center], dim=-1).float()
            max_m = self.center
            for d in range(8):
                v = self.dirs[d]
                proj = rel[..., 0] * v[1] + rel[..., 1] * v[0]
                step = proj.round().clamp(min=-max_m, max=max_m).int()
                pos_masks, neg_masks = [], []
                for m in range(max_m + 1):
                    pos_masks.append((step == m).float())
                    neg_masks.append((step == -m).float())
                base.append(torch.stack(pos_masks, 0))
                base.append(torch.stack(neg_masks, 0))
        dir_step_base = torch.stack([torch.stack(base[2*i:2*i+2], 0) for i in range(8)], 0)
        self.register_buffer('dir_step_base', dir_step_base)

    @staticmethod
    def gl_coeffs(v, M):
        if v.dim() == 3: v = v.unsqueeze(1)
        B, _, H, W = v.shape
        coeffs = [torch.ones(B,1,H,W, device=v.device, dtype=v.dtype)]
        w = coeffs[0]
        for m in range(1, M+1):
            w = w * (v - m + 1.0)/m * (-1.0)
            coeffs.append(w)
        return torch.cat(coeffs, dim=1)

    def _direction_kernel(self, v_dir, d):
        M = self.center
        coeff = self.gl_coeffs(v_dir, M)  # [B,M+1,H,W]
        base_pos = self.dir_step_base[d,0]  # [M+1,K,K]
        base_neg = self.dir_step_base[d,1]
        ker = (coeff[:, :, None, None, ...] *
               (base_pos[None, :, :, :, None, None] - base_neg[None, :, :, :, None, None])).sum(1)
        return ker  # [B,K,K,H,W]

    def _build_kernels(self, v):
        ks = [self._direction_kernel(v[:, d:d+1], d) for d in range(8)]
        return torch.stack(ks, 0).mean(0)  # [B,K,K,H,W]

    def forward(self, x):
        B, C, h, w = x.shape
        raw_v = self.order_head(x)
        g = self.gate(x)                         # [B,1,h,w]
        v = self.vmax * torch.tanh(raw_v)        # [B,8,h,w]
        ker = self._build_kernels(v)             # [B,K,K,h,w]
        K, pad = self.k, self.center

        # ä½é€šæƒé‡ï¼ˆä¸­å¿ƒå…ˆéªŒ + æ¸©åº¦ softmaxï¼‰
        mask_pre = ker.view(B, K*K, h, w)
        center_idx = self.center * self.k + self.center
        mask_pre[:, center_idx:center_idx+1] += self.center_bias
        mask_lp = torch.softmax(mask_pre / self.tau, dim=1)

        # é‚»åŸŸå±•å¼€
        xp = F.pad(x, [pad, pad, pad, pad], mode='reflect')
        patches = F.unfold(xp, kernel_size=K, stride=1, padding=0).view(B, C, K*K, h, w)

        # æ”¾å¤§ç©ºé—´ç»´
        if self.s > 1:
            neigh   = patches.repeat_interleave(self.s, dim=3).repeat_interleave(self.s, dim=4)  # [B,C,K^2,H,W]
            mask_lp = mask_lp.repeat_interleave(self.s, dim=2).repeat_interleave(self.s, dim=3)  # [B,K^2,H,W]
            g_up    = g.repeat_interleave(self.s, dim=2).repeat_interleave(self.s, dim=3)        # [B,1,H,W]
            x_up    = F.interpolate(x, scale_factor=self.s, mode='bilinear', align_corners=True) # [B,C,H,W]
        else:
            neigh, g_up, x_up = patches, g, x

        # ä½é€šè¾“å‡º + è¾¹ç¼˜è‡ªé€‚åº”æ®‹å·®
        y_lp_out = (neigh * mask_lp[:, None]).sum(dim=2)
        lambda_eff = self.smooth_residual * (1.0 - g_up)
        y_lp = x_up + lambda_eff * (y_lp_out - x_up)

        if self.mode == 'lowpass':
            return y_lp, v, g_up

        # é«˜é€šåˆ†æ”¯ï¼ˆé›¶ç›´æµ + L1å½’ä¸€ï¼‰
        ker_hp = ker - ker.mean(dim=(1, 2), keepdim=True)
        ker_hp = ker_hp / (ker_hp.abs().sum(dim=(1, 2), keepdim=True) + 1e-8)
        mask_hp = ker_hp.view(B, K*K, h, w)
        if self.s > 1:
            mask_hp = mask_hp.repeat_interleave(self.s, dim=2).repeat_interleave(self.s, dim=3)
        y_hp = (neigh * mask_hp[:, None]).sum(dim=2)

        if self.mode == 'highpass':
            return y_hp, v, g_up

        y = y_lp + torch.tanh(self.beta) * g_up * y_hp
        return y, v, g_up


# In[ ]:


# ç”Ÿæˆåˆ†æ•°é˜¶ä¸Šé‡‡æ ·æ¨¡å—ï¼šmodel/modules/fractional_fusion.py
import os, pathlib

repo = '/kaggle/working/UniMatch'
dst  = os.path.join(repo, 'model', 'modules')
os.makedirs(dst, exist_ok=True)
path = os.path.join(dst, 'fractional_fusion.py')

content = r"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class FractionalFusionUp(nn.Module):
    '''
    åˆ†æ•°é˜¶è‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼ˆæ›¿ä»£æœ€ç»ˆ logits çš„åŒçº¿æ€§ä¸Šé‡‡æ ·ï¼‰
    - ä»…æ”¾å¤§ç©ºé—´ç»´åº¦ï¼Œä¸æ”¹å˜é€šé“æ•°
    - 8æ–¹å‘åˆ†æ•°é˜¶æ ¸ + é—¨æ§é«˜é€šå¢å¼º + ä½é€šæ®‹å·®èåˆ
    è¿”å›: y [B,C,H,W], orders [B,8,h,w], gate [B,1,H,W]
    '''
    def __init__(self, in_channels, up_factor=4, kernel_size=3, vmax=1.0, hidden=48,
                 mode='both', beta=1.6, tau=0.5, center_bias=2.5, smooth_residual=0.15):
        super().__init__()
        self.k = int(kernel_size)
        self.s = int(up_factor)
        self.vmax = float(vmax)
        self.mode = mode
        self.center = self.k // 2
        self.beta = nn.Parameter(torch.tensor(float(beta)))
        self.tau = float(tau)
        self.center_bias = float(center_bias)
        self.smooth_residual = float(smooth_residual)

        # 8ä¸ªæ–¹å‘å•ä½å‘é‡
        dirs = torch.tensor([[1,0],[1,1],[0,1],[-1,1],[-1,0],[-1,-1],[0,-1],[1,-1]], dtype=torch.float32)
        self.register_buffer('dirs', dirs / (dirs.norm(dim=1, keepdim=True) + 1e-8))

        # é¢„æµ‹åˆ†æ•°é˜¶æ¬¡ä¸é—¨æ§ï¼ˆåœ¨ç±»åˆ«é€šé“ä¸Šï¼‰
        self.order_head = nn.Sequential(
            nn.Conv2d(in_channels, hidden, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(hidden, 8, 3, 1, 1)
        )
        self.gate = nn.Sequential(
            nn.Conv2d(in_channels, hidden, 3, 1, 1), nn.ReLU(inplace=True),
            nn.Conv2d(hidden, 1, 3, 1, 1), nn.Sigmoid()
        )

        # é¢„è®¡ç®—æ–¹å‘-æ­¥é•¿åŸºåº• [8,2,M+1,K,K]
        base = []
        with torch.no_grad():
            yy, xx = torch.meshgrid(torch.arange(self.k), torch.arange(self.k), indexing='ij')
            rel = torch.stack([yy - self.center, xx - self.center], dim=-1).float()
            max_m = self.center
            for d in range(8):
                v = self.dirs[d]
                proj = rel[..., 0] * v[1] + rel[..., 1] * v[0]
                step = proj.round().clamp(min=-max_m, max=max_m).int()
                pos_masks, neg_masks = [], []
                for m in range(max_m + 1):
                    pos_masks.append((step == m).float())
                    neg_masks.append((step == -m).float())
                base.append(torch.stack(pos_masks, 0))
                base.append(torch.stack(neg_masks, 0))
        dir_step_base = torch.stack([torch.stack(base[2*i:2*i+2], 0) for i in range(8)], 0)
        self.register_buffer('dir_step_base', dir_step_base)

    @staticmethod
    def gl_coeffs(v, M):
        # Grunwaldâ€“Letnikov ç³»æ•°
        if v.dim() == 3:
            v = v.unsqueeze(1)
        B, _, H, W = v.shape
        coeffs = [torch.ones(B,1,H,W, device=v.device, dtype=v.dtype)]
        w = coeffs[0]
        for m in range(1, M+1):
            w = w * (v - m + 1.0)/m * (-1.0)
            coeffs.append(w)
        return torch.cat(coeffs, dim=1)  # [B,M+1,H,W]

    def _direction_kernel(self, v_dir, d):
        M = self.center
        coeff = self.gl_coeffs(v_dir, M)  # [B,M+1,H,W]
        base_pos = self.dir_step_base[d,0]  # [M+1,K,K]
        base_neg = self.dir_step_base[d,1]  # [M+1,K,K]
        ker = (coeff[:, :, None, None, ...] *
               (base_pos[None, :, :, :, None, None] - base_neg[None, :, :, :, None, None])).sum(1)
        return ker  # [B,K,K,H,W]

    def _build_kernels(self, v):
        # 8æ–¹å‘å–å‡å€¼
        ks = [self._direction_kernel(v[:, d:d+1], d) for d in range(8)]
        return torch.stack(ks, 0).mean(0)  # [B,K,K,H,W]

    def forward(self, x):
        B, C, h, w = x.shape
        raw_v = self.order_head(x)
        g = self.gate(x)                         # [B,1,h,w]
        v = self.vmax * torch.tanh(raw_v)        # [B,8,h,w]
        ker = self._build_kernels(v)             # [B,K,K,h,w]
        K, pad = self.k, self.center

        # ä½é€šæƒé‡ï¼ˆä¸­å¿ƒåç½® + æ¸©åº¦ softmaxï¼‰
        mask_pre = ker.view(B, K*K, h, w)
        center_idx = self.center * self.k + self.center
        mask_pre[:, center_idx:center_idx+1] = mask_pre[:, center_idx:center_idx+1] + self.center_bias
        mask_lp = torch.softmax(mask_pre / self.tau, dim=1)  # [B,K^2,h,w]

        # é‚»åŸŸå±•å¼€
        xp = F.pad(x, [pad, pad, pad, pad], mode='reflect')
        patches = F.unfold(xp, kernel_size=K, stride=1, padding=0).view(B, C, K*K, h, w)

        # æ”¾å¤§ç©ºé—´ç»´ï¼ˆä»…ç©ºé—´ï¼Œä¿ç•™é€šé“ï¼‰
        if self.s > 1:
            neigh   = patches.repeat_interleave(self.s, dim=3).repeat_interleave(self.s, dim=4)  # [B,C,K^2,H,W]
            mask_lp = mask_lp.repeat_interleave(self.s, dim=2).repeat_interleave(self.s, dim=3)  # [B,K^2,H,W]
            g_up    = g.repeat_interleave(self.s, dim=2).repeat_interleave(self.s, dim=3)        # [B,1,H,W]
            x_up    = F.interpolate(x, scale_factor=self.s, mode='bilinear', align_corners=False) # [B,C,H,W]
        else:
            neigh, g_up, x_up = patches, g, x

        # ä½é€šè¾“å‡º + è¾¹ç¼˜è‡ªé€‚åº”æ®‹å·®ï¼ˆé™ä½æ¨¡ç³Šï¼‰
        y_lp_out = (neigh * mask_lp[:, None]).sum(dim=2)
        lambda_eff = self.smooth_residual * (1.0 - g_up)
        y_lp = x_up + lambda_eff * (y_lp_out - x_up)

        if self.mode == 'lowpass':
            return y_lp, v, g_up

        # é«˜é€šï¼ˆé›¶ç›´æµ + L1å½’ä¸€ï¼‰
        ker_hp = ker - ker.mean(dim=(1, 2), keepdim=True)
        ker_hp = ker_hp / (ker_hp.abs().sum(dim=(1, 2), keepdim=True) + 1e-8)
        mask_hp = ker_hp.view(B, K*K, h, w)
        if self.s > 1:
            mask_hp = mask_hp.repeat_interleave(self.s, dim=2).repeat_interleave(self.s, dim=3)
        y_hp = (neigh * mask_hp[:, None]).sum(dim=2)

        if self.mode == 'highpass':
            return y_hp, v, g_up

        # èåˆ
        y = y_lp + torch.tanh(self.beta) * g_up * y_hp
        return y, v, g_up
"""

pathlib.Path(path).write_text(content, encoding='utf-8')
print(f'âœ… å·²ç”Ÿæˆ: {path}')

# ç®€å•æ ¡éªŒ
try:
    from model.modules.fractional_fusion import FractionalFusionUp
    m = FractionalFusionUp(in_channels=21, up_factor=4)
    import torch
    y, _, _ = m(torch.randn(1,21,64,64))
    print('âœ… æ¨¡å—å¯¼å…¥ä¸å‰å‘é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶:', y.shape)
except Exception as e:
    print('âŒ æ ¡éªŒå¤±è´¥:', e)


# In[ ]:


import os, re, pathlib, sys

repo = '/kaggle/working/UniMatch'
os.chdir(repo)

# æŸ¥æ‰¾ deeplabv3plus.py
candidate = None
for root, _, files in os.walk(os.path.join(repo, 'model')):
    for f in files:
        if f.lower() == 'deeplabv3plus.py' and 'semseg' in root.replace('\\','/'):
            candidate = os.path.join(root, f)
            break
    if candidate: break

assert candidate and os.path.exists(candidate), f'æœªæ‰¾åˆ° deeplabv3plus.pyï¼Œå½“å‰ repo: {repo}'
print('ç›®æ ‡æ–‡ä»¶:', candidate)

src = pathlib.Path(candidate).read_text(encoding='utf-8')
orig = src

# 1) ç¡®ä¿æœ‰ F å’Œæˆ‘ä»¬çš„æ¨¡å—å¯¼å…¥
if 'import torch.nn.functional as F' not in src:
    src = src.replace('from torch import nn', 'from torch import nn\nimport torch.nn.functional as F')

if 'from model.modules.fractional_fusion import FractionalFusionUp' not in src:
    # å°è¯•åœ¨ F.import åé¢æ’å…¥
    if 'import torch.nn.functional as F' in src:
        src = src.replace('import torch.nn.functional as F',
                          'import torch.nn.functional as F\nfrom model.modules.fractional_fusion import FractionalFusionUp')
    else:
        # å…œåº•æ’åˆ°æ–‡ä»¶é¡¶éƒ¨
        src = 'from model.modules.fractional_fusion import FractionalFusionUp\n' + src

# 2) åœ¨ class DeepLabV3Plus å†…æ³¨å…¥ _up æ–¹æ³•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
cls_pat = re.compile(r'(class\s+DeepLabV3Plus\s*\(.*?\)\s*:\s*)', re.S)
m = cls_pat.search(src)
assert m, 'æœªæ‰¾åˆ°ç±» DeepLabV3Plus å®šä¹‰'

if 'def _up(self,' not in src:
    inject = """
    def _up(self, t, size=None, scale_factor=None, mode="bilinear", align_corners=True):
        \"\"\"å®‰å…¨ä¸Šé‡‡æ ·ï¼šä»…åœ¨é€šé“æ•°ç­‰äºç±»åˆ«æ•°æ—¶å¯ç”¨åˆ†æ•°é˜¶ï¼Œå¦åˆ™å›é€€åŒçº¿æ€§\"\"\"
        try:
            nclass = getattr(self, "nclass", None)
            if nclass is None and hasattr(self, "classifier"):
                nclass = getattr(self.classifier, "out_channels", None)
            if nclass is None:
                nclass = t.shape[1]
            is_logits = (t.shape[1] == nclass)
            if is_logits:
                # æ‡’åˆå§‹åŒ–åˆ†æ•°é˜¶ä¸Šé‡‡æ ·ï¼ˆæ ¹æ®å½“å‰å€ç‡æ¨æ–­ up_factorï¼‰
                if not hasattr(self, "frac_up") or self.frac_up is None:
                    if scale_factor is not None:
                        s = int(scale_factor)
                    elif size is not None:
                        s = max(1, int(round((size[0] if isinstance(size, (list, tuple)) else size) / t.shape[-2])))
                    else:
                        s = 4
                    s = max(1, s)
                    self.frac_up = FractionalFusionUp(
                        in_channels=nclass, up_factor=s, kernel_size=3,
                        vmax=1.0, hidden=48, mode="both", beta=1.6,
                        tau=0.5, center_bias=2.5, smooth_residual=0.15
                    )
                y, _, _ = self.frac_up(t)
                if size is not None and tuple(y.shape[-2:]) != tuple(size):
                    y = F.interpolate(y, size=size, mode=mode, align_corners=align_corners)
                return y
        except Exception:
            pass
        return F.interpolate(t, size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)
    """
    # æ³¨å…¥åˆ°ç±»å®šä¹‰å
    insert_at = m.end()
    src = src[:insert_at] + '\n' + inject + '\n' + src[insert_at:]

# 3) å…¨å±€æ›¿æ¢ F.interpolate ä¸º self._up
src = re.sub(r'\bF\.interpolate\s*\(', 'self._up(', src)

# å†™å…¥å¤‡ä»½ä¸æ–°æ–‡ä»¶
if src != orig:
    backup = candidate + '.bak'
    pathlib.Path(backup).write_text(orig, encoding='utf-8')
    pathlib.Path(candidate).write_text(src, encoding='utf-8')
    print('âœ… å·²æ‰“è¡¥ä¸ï¼Œå¤‡ä»½æ–‡ä»¶:', backup)
else:
    print('â„¹ï¸ æ— å˜åŒ–ï¼ˆå¯èƒ½ä¹‹å‰å·²æ‰“è¿‡è¡¥ä¸ï¼‰')


# In[ ]:


# å¿«é€Ÿå†™å…¥é…ç½®ï¼ˆå¦‚å·²æœ‰å­—æ®µåˆ™è¦†ç›–ï¼‰
import yaml, os
cfg_path = 'configs/pascal.yaml'
with open(cfg_path, 'r') as f: cfg = yaml.safe_load(f)
cfg.update({
    'use_fractional_up': True,
    'frac_up_factor': 4,            # logits çš„ä¸‹é‡‡æ ·å€ç‡ï¼ŒDeepLabV3+ å¸¸è§ä¸º4
    'frac_kernel_size': 3,
    'frac_vmax': 1.0,
    'frac_beta': 1.6,
    'frac_hidden': 48,
    'frac_tau': 0.5,
    'frac_center_bias': 2.5,
    'frac_smooth_residual': 0.15
})
with open(cfg_path, 'w') as f: yaml.dump(cfg, f)
print('âœ… å·²æ›´æ–° configs/pascal.yaml')


# In[ ]:


import os, re, pathlib, textwrap

repo = '/kaggle/working/UniMatch'
os.chdir(repo)

# 1) æ‰¾åˆ° deeplabv3plus.py
target = None
for root, _, files in os.walk(os.path.join(repo, 'model')):
    for f in files:
        if f.lower() == 'deeplabv3plus.py' and 'semseg' in root.replace('\\','/'):
            target = os.path.join(root, f)
            break
    if target: break

assert target and os.path.exists(target), f'æœªæ‰¾åˆ° deeplabv3plus.pyï¼›å½“å‰ repo: {repo}'
print('ç›®æ ‡æ–‡ä»¶:', target)

# 2) è‹¥æœ‰ .bakï¼Œå›æ»šåˆ° .bak å†é‡æ‰“è¡¥ä¸ï¼ˆé¿å…ä¸Šä¸€æ¬¡æ‰“åç¼©è¿›ï¼‰
bak = target + '.bak'
if os.path.exists(bak):
    print('å‘ç°å¤‡ä»½ï¼Œå…ˆå›æ»š .bak å†è¡¥ä¸')
    pathlib.Path(target).write_text(pathlib.Path(bak).read_text(encoding='utf-8'), encoding='utf-8')

src = pathlib.Path(target).read_text(encoding='utf-8')

# 3) ç»Ÿä¸€ç¼©è¿›ï¼ˆåˆ¶è¡¨ç¬¦ â†’ 4ç©ºæ ¼ï¼‰ï¼Œç»Ÿä¸€è¡Œå°¾
src = src.replace('\r\n', '\n').replace('\r', '\n')
src = src.replace('\t', '    ')

# 4) ç¡®ä¿ import F ä¸ FractionalFusionUp
if 'import torch.nn.functional as F' not in src:
    # æ’åœ¨ from torch import nn ä¹‹åï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™é¡¶ç«¯æ’å…¥
    if 'from torch import nn' in src:
        src = src.replace('from torch import nn', 'from torch import nn\nimport torch.nn.functional as F')
    else:
        src = 'import torch.nn.functional as F\n' + src

if 'from model.modules.fractional_fusion import FractionalFusionUp' not in src:
    src = src.replace(
        'import torch.nn.functional as F',
        'import torch.nn.functional as F\nfrom model.modules.fractional_fusion import FractionalFusionUp'
    )

# 5) æ‰¾åˆ° DeepLabV3Plus ç±»ä½“èŒƒå›´
class_pat = re.compile(r'\nclass\s+DeepLabV3Plus\s*\(.*?\)\s*:\s*\n', re.S)
m = class_pat.search(src)
assert m, 'æœªæ‰¾åˆ°ç±» DeepLabV3Plus å®šä¹‰'
cls_start = m.end()

# æŸ¥æ‰¾ç±»ç»“æŸï¼ˆä¸‹ä¸€ä¸ª class/def åœ¨åˆ—é¦–ï¼Œæˆ–æ–‡ä»¶ç»“å°¾ï¼‰
tail_pat = re.compile(r'\n(class\s+|def\s+)', re.S)
m_end = tail_pat.search(src, cls_start)
cls_end = m_end.start()+1 if m_end else len(src)

cls_block = src[cls_start:cls_end]

# 6) æ³¨å…¥ _upï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰ï¼Œç”¨ä¸ç±»ä½“ä¸€è‡´çš„ç¼©è¿›
if 'def _up(' not in cls_block:
    # ä¼°è®¡ç±»ä½“ç¼©è¿›ï¼šæ‰¾ç¬¬ä¸€æ¡éç©ºè¡Œçš„å‰å¯¼ç©ºæ ¼ï¼›é»˜è®¤ 4 ç©ºæ ¼
    lines_after = cls_block.splitlines()
    indent_cls = '    '
    for ln in lines_after:
        if ln.strip():
            indent_cls = ln[:len(ln)-len(ln.lstrip(' '))] or '    '
            break

    up_code = textwrap.dedent("""
    def _up(self, t, size=None, scale_factor=None, mode="bilinear", align_corners=False):
        \"\"\"ä»…å½“é€šé“æ•°ç­‰äºç±»åˆ«æ•°æ—¶å¯ç”¨åˆ†æ•°é˜¶ä¸Šé‡‡æ ·ï¼›å¦åˆ™å›é€€åŒçº¿æ€§\"\"\"
        try:
            nclass = getattr(self, "nclass", None)
            if nclass is None and hasattr(self, "classifier"):
                nclass = getattr(self.classifier, "out_channels", None)
            if nclass is None:
                nclass = t.shape[1]
            is_logits = (t.shape[1] == nclass)

            if is_logits:
                # æ‡’åˆå§‹åŒ–åˆ†æ•°é˜¶ä¸Šé‡‡æ ·
                if not hasattr(self, "frac_up") or self.frac_up is None:
                    if scale_factor is not None:
                        s = int(scale_factor)
                    elif size is not None:
                        tgt_h = size[0] if isinstance(size, (list, tuple)) else size
                        s = max(1, int(round(tgt_h / t.shape[-2])))
                    else:
                        s = 4
                    s = max(1, s)
                    self.frac_up = FractionalFusionUp(
                        in_channels=nclass, up_factor=s, kernel_size=3,
                        vmax=1.0, hidden=48, mode="both", beta=1.6,
                        tau=0.5, center_bias=2.5, smooth_residual=0.15
                    )
                y, _, _ = self.frac_up(t)
                if size is not None and tuple(y.shape[-2:]) != tuple(size):
                    y = F.interpolate(y, size=size, mode=mode, align_corners=align_corners)
                return y
        except Exception:
            # ä»»æ„å¼‚å¸¸éƒ½å›é€€åŒçº¿æ€§ï¼Œä¿è¯è®­ç»ƒä¸è¢«æ‰“æ–­
            pass
        return F.interpolate(t, size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)
    """).strip('\n')

    # ç¼©è¿›åˆ°ç±»ä½“
    up_code_indented = '\n'.join(
        (indent_cls + ln) if ln.strip() else ln
        for ln in up_code.splitlines()
    )
    # æ’åˆ°ç±»ä½“å¼€å¤´
    cls_block = up_code_indented + '\n\n' + cls_block

# 7) ä»…åœ¨ç±»ä½“å†…å°† F.interpolate( æ›¿æ¢ä¸º self._up(
cls_block = re.sub(r'\bF\.interpolate\s*\(', 'self._up(', cls_block)

# 8) å›å†™æ–‡ä»¶ï¼ˆå¤‡ä»½ä¸€æ¬¡ï¼‰
orig = pathlib.Path(target).read_text(encoding='utf-8')
backup = target + '.bak'
pathlib.Path(backup).write_text(orig, encoding='utf-8')
new_src = src[:cls_start] + cls_block + src[cls_start+len(src[cls_start:cls_end]):]
pathlib.Path(target).write_text(new_src, encoding='utf-8')
print('âœ… ä¿®å¤å®Œæˆï¼Œå·²å†™å…¥å¹¶å¤‡ä»½åˆ°:', backup)

# 9) ç®€è¦æç¤º
if not os.path.exists('model/modules/fractional_fusion.py'):
    print('âš ï¸ æœªæ£€æµ‹åˆ° model/modules/fractional_fusion.pyï¼Œè¯·å…ˆåˆ›å»ºè¯¥æ¨¡å—æ–‡ä»¶åå† dry-runã€‚')


# In[ ]:


# ä¿®å¤ _up æ–¹æ³•å†…è¢«è¯¯æ›¿æ¢æˆ self._up(...) çš„è°ƒç”¨ï¼Œç»Ÿä¸€è¿˜åŸä¸º F.interpolate(...)
import os, re, pathlib

repo = '/kaggle/working/UniMatch'
os.chdir(repo)

# å®šä½ deeplabv3plus.py
target = None
for root, _, files in os.walk(os.path.join(repo, 'model')):
    for f in files:
        if f.lower() == 'deeplabv3plus.py' and 'semseg' in root.replace('\\','/'):
            target = os.path.join(root, f)
            break
    if target: break
assert target and os.path.exists(target), 'æœªæ‰¾åˆ° deeplabv3plus.py'

src = pathlib.Path(target).read_text(encoding='utf-8')

# æå–å¹¶ä¿®å¤ _up æ–¹æ³•ä½“ï¼ˆæŠŠ self._up( è¿˜åŸæˆ F.interpolate( ï¼‰
m = re.search(r'(def\s+_up\s*\(.*?\):\s*)([\s\S]*?)(?=\n\s{4}def\s+|\n\s*class\s+|\Z)', src)
assert m, 'æœªåœ¨æ–‡ä»¶å†…æ‰¾åˆ° _up æ–¹æ³•ï¼ˆè¯·å…ˆæ‰§è¡Œâ€œè‡ªä¿®å¤è¡¥ä¸â€æ³¨å…¥ _upï¼‰'

head, body = m.group(1), m.group(2)
fixed_body, cnt = re.subn(r'\bself\._up\s*\(', 'F.interpolate(', body)
if cnt == 0:
    print('â„¹ï¸ _up å†…æœªå‘ç° self._up è°ƒç”¨ï¼Œæ— éœ€ä¿®æ”¹')
else:
    print(f'âœ… å·²ä¿®å¤ _up å†… {cnt} å¤„é€’å½’è°ƒç”¨')

# å†™å›æ–‡ä»¶å¹¶å¤‡ä»½
backup = target + '.bak2'
pathlib.Path(backup).write_text(src, encoding='utf-8')
new_src = src[:m.start(2)] + fixed_body + src[m.end(2):]
pathlib.Path(target).write_text(new_src, encoding='utf-8')
print('å†™å…¥å®Œæˆï¼Œå¤‡ä»½åˆ°:', backup)


# In[ ]:


import os, yaml, torch, importlib
os.chdir('/kaggle/working/UniMatch')

import model.semseg.deeplabv3plus as dlv3p
importlib.reload(dlv3p)  # é‡æ–°åŠ è½½ï¼Œåº”ç”¨åˆšåˆšçš„ä¿®å¤
from model.semseg.deeplabv3plus import DeepLabV3Plus

with open('configs/pascal.yaml','r') as f:
    cfg = yaml.safe_load(f)

m = DeepLabV3Plus(cfg).eval()
x = torch.randn(1,3,321,321)
with torch.no_grad():
    y = m(x)
print('input:', x.shape, 'output:', y.shape)


# In[ ]:


# Cell 3: ä¿®æ­£ç‰ˆ - ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•ä¸‹ä½¿ç”¨ torchrun

import yaml
import os

# --- ç¡®ä¿æˆ‘ä»¬åœ¨æ­£ç¡®çš„å·¥ä½œç›®å½• ---
project_dir = '/kaggle/working/UniMatch'
os.chdir(project_dir)
print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# è¯»å–å·²ç»æ›´æ–°çš„é…ç½®ï¼ˆåŒ…å«å¢å¼ºåçš„æ•°æ®è·¯å¾„ï¼‰
config_file_path = 'configs/pascal.yaml'
with open(config_file_path, 'r') as f:
    config_data = yaml.safe_load(f)

print(f"ä½¿ç”¨å¢å¼ºåçš„æ•°æ®é›†: {config_data['data_root']}")


# --- ç¬¬ 1 æ­¥: å¤ç° train.sh ä¸­çš„å˜é‡è®¾ç½® ---
dataset = 'pascal'
method = 'unimatch'
exp = 'r101'
split = '732'
num_gpus = 1

# --- ç¬¬ 2 æ­¥: å®šä¹‰å’Œä¿®æ­£æ‰€æœ‰è·¯å¾„ ---
config_file_path = f'configs/{dataset}.yaml'

labeled_id_path = f'splits/{dataset}/{split}/labeled.txt'  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
unlabeled_id_path = f'splits/{dataset}/{split}/unlabeled.txt'  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
save_path = f'/kaggle/working/exp/{dataset}/{method}/{exp}/{split}'
os.makedirs(save_path, exist_ok=True)

port = 12345

# --- ç¬¬ 3 æ­¥: åŠ¨æ€ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ data_root ---
print(f"å‡†å¤‡ä¿®æ”¹é…ç½®æ–‡ä»¶: {config_file_path}")
with open(config_file_path, 'r') as f:
    config_data = yaml.safe_load(f)

print(f"åŸ data_root: {config_data.get('data_root', 'æœªæ‰¾åˆ°')}")
print(f"æ–° data_root: {config_data['data_root']}")
config_data['data_root'] = '/kaggle/input/pascal1-3/VOC2012'
with open(config_file_path, 'w') as f:
    yaml.dump(config_data, f)
print("é…ç½®æ–‡ä»¶ä¿®æ”¹æˆåŠŸï¼")

# --- ç¬¬ 4 æ­¥: éªŒè¯å…³é”®æ–‡ä»¶å­˜åœ¨ ---
print("\néªŒè¯å…³é”®æ–‡ä»¶:")
files_to_check = [
    f'{method}.py',
    config_file_path,
    labeled_id_path,
    unlabeled_id_path
]

for file_path in files_to_check:
    if os.path.exists(file_path):
        print(f"âœ… {file_path}")
    else:
        print(f"âŒ {file_path} - æœªæ‰¾åˆ°!")

# --- ç¬¬ 5 æ­¥: ä½¿ç”¨ torchrun å¯åŠ¨åŒGPUè®­ç»ƒ ---
print(f"\nå°†ä½¿ç”¨ {num_gpus} ä¸ª GPU è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ torchrun)...")

# ä½¿ç”¨ç›¸å¯¹è·¯å¾„è°ƒç”¨ unimatch.pyï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨é¡¹ç›®ç›®å½•ä¸­
get_ipython().system('torchrun      --nproc_per_node={num_gpus}      --master_port={port}      {method}.py      --config {config_file_path}      --labeled-id-path {labeled_id_path}      --unlabeled-id-path {unlabeled_id_path}      --save-path {save_path}      --port {port}')

print("\nåˆ†å¸ƒå¼è®­ç»ƒå‘½ä»¤å·²æ‰§è¡Œã€‚")


# In[ ]:


# ä¿®å¤ Cell: è®­ç»ƒç»“æœå¯è§†åŒ–ï¼ˆPyTorch 2.6å…¼å®¹ + æ•°æ®è·¯å¾„ä¿®å¤ï¼‰

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import yaml
import random
from torch.utils.data import DataLoader
import sys

# ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
os.chdir('/kaggle/working/UniMatch')

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
sys.path.append('/kaggle/working/UniMatch')

def load_trained_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""

    # è¯»å–é…ç½®
    config_file_path = 'configs/pascal.yaml'
    with open(config_file_path, 'r') as f:
        cfg = yaml.safe_load(f)

    print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    print(f"Backbone: {cfg.get('backbone', 'unknown')}")
    print(f"Model: {cfg.get('model', 'unknown')}")
    print(f"Data root: {cfg.get('data_root', 'unknown')}")

    # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹checkpoint
    save_path = '/kaggle/working/exp/pascal/unimatch/r101/732'

    # æŸ¥æ‰¾æœ€æ–°çš„.pthæ–‡ä»¶
    pth_files = []
    if os.path.exists(save_path):
        for file in os.listdir(save_path):
            if file.endswith('.pth'):
                pth_files.append(os.path.join(save_path, file))

    if not pth_files:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼")
        return None, None

    # é€‰æ‹©æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    latest_model = max(pth_files, key=os.path.getctime)
    print(f"åŠ è½½æ¨¡å‹: {latest_model}")

    # ä¿®å¤PyTorch 2.6å…¼å®¹æ€§é—®é¢˜
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨ weights_only=False
        checkpoint = torch.load(latest_model, map_location='cpu', weights_only=False)
        print("âœ… ä½¿ç”¨ weights_only=False æˆåŠŸåŠ è½½")
    except Exception as e:
        print(f"âŒ weights_only=False å¤±è´¥: {e}")
        try:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ pickle æ¨¡å—ç›´æ¥åŠ è½½
            import pickle
            with open(latest_model, 'rb') as f:
                checkpoint = pickle.load(f)
            print("âœ… ä½¿ç”¨ pickle æˆåŠŸåŠ è½½")
        except Exception as e2:
            print(f"âŒ æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {e2}")
            return None, None

    # æ£€æŸ¥checkpointçš„ç»“æ„
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
        print("æ‰¾åˆ° 'model' é”®")
    else:
        state_dict = checkpoint
        print("ç›´æ¥ä½¿ç”¨checkpointä½œä¸ºstate_dict")

    # åŠ¨æ€åˆ›å»ºæ¨¡å‹
    model = None

    try:
        from model.semseg.deeplabv3plus import DeepLabV3Plus
        model = DeepLabV3Plus(cfg)
        print("âœ… ä½¿ç”¨ DeepLabV3Plus åˆ›å»ºæ¨¡å‹")
    except Exception as e:
        print(f"âŒ DeepLabV3Plus åˆ›å»ºå¤±è´¥: {e}")
        return None, None

    # å°è¯•åŠ è½½æƒé‡
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¿å­˜çš„æ¨¡å‹
        if any(key.startswith('module.') for key in state_dict.keys()):
            print("æ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å‹ï¼Œç§»é™¤ 'module.' å‰ç¼€")
            new_state_dict = {}
            for key, value in state_dict.items():
                if key.startswith('module.'):
                    new_key = key[7:]  # ç§»é™¤ 'module.' å‰ç¼€
                    new_state_dict[new_key] = value
                else:
                    new_state_dict[key] = value
            state_dict = new_state_dict

        # åŠ è½½æƒé‡
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")

        if missing_keys:
            print(f"âš ï¸  ç¼ºå°‘ {len(missing_keys)} ä¸ªé”®")
        if unexpected_keys:
            print(f"âš ï¸  å¤šä½™ {len(unexpected_keys)} ä¸ªé”®")

        model.eval()
        if torch.cuda.is_available():
            model.cuda()
            print("âœ… æ¨¡å‹å·²ç§»è‡³GPU")

        return model, cfg

    except Exception as e:
        print(f"âŒ åŠ è½½æƒé‡å¤±è´¥: {e}")
        return None, None

def create_simple_test_dataset(cfg):
    """åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®é›†"""

    # æ£€æŸ¥æ•°æ®æ ¹ç›®å½•
    data_root = cfg['data_root']
    print(f"æ•°æ®æ ¹ç›®å½•: {data_root}")

    # å°è¯•ä¸åŒçš„è·¯å¾„ç»„åˆ
    possible_img_dirs = [
        os.path.join(data_root, 'JPEGImages'),
        os.path.join(data_root, 'images'),
        os.path.join(data_root, 'img'),
    ]

    possible_mask_dirs = [
        os.path.join(data_root, 'SegmentationClass'),
        os.path.join(data_root, 'annotations'),
        os.path.join(data_root, 'masks'),
    ]

    val_img_dir = None
    val_mask_dir = None

    # æŸ¥æ‰¾å›¾ç‰‡ç›®å½•
    for img_dir in possible_img_dirs:
        if os.path.exists(img_dir):
            val_img_dir = img_dir
            print(f"âœ… æ‰¾åˆ°å›¾ç‰‡ç›®å½•: {val_img_dir}")
            break

    # æŸ¥æ‰¾æ ‡ç­¾ç›®å½•  
    for mask_dir in possible_mask_dirs:
        if os.path.exists(mask_dir):
            val_mask_dir = mask_dir
            print(f"âœ… æ‰¾åˆ°æ ‡ç­¾ç›®å½•: {val_mask_dir}")
            break

    if not val_img_dir:
        print(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡ç›®å½•ï¼Œå°è¯•çš„è·¯å¾„:")
        for dir_path in possible_img_dirs:
            print(f"   - {dir_path}")
        return []

    if not val_mask_dir:
        print(f"âŒ æœªæ‰¾åˆ°æ ‡ç­¾ç›®å½•ï¼Œå°è¯•çš„è·¯å¾„:")
        for dir_path in possible_mask_dirs:
            print(f"   - {dir_path}")
        return []

    # è·å–å›¾ç‰‡åˆ—è¡¨
    try:
        all_img_files = [f for f in os.listdir(val_img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"æ‰¾åˆ° {len(all_img_files)} å¼ å›¾ç‰‡")

        if len(all_img_files) == 0:
            print("âŒ å›¾ç‰‡ç›®å½•ä¸ºç©º")
            return []

        # éšæœºé€‰æ‹©å‡ å¼ å›¾ç‰‡
        random.seed(42)
        selected_files = random.sample(all_img_files, min(5, len(all_img_files)))

        test_data = []
        for img_file in selected_files:
            img_path = os.path.join(val_img_dir, img_file)

            # å°è¯•ä¸åŒçš„æ ‡ç­¾æ–‡ä»¶æ‰©å±•å
            base_name = os.path.splitext(img_file)[0]
            possible_mask_files = [
                base_name + '.png',
                base_name + '.jpg', 
                img_file.replace('.jpg', '.png'),
                img_file.replace('.jpeg', '.png')
            ]

            mask_path = None
            for mask_file in possible_mask_files:
                potential_path = os.path.join(val_mask_dir, mask_file)
                if os.path.exists(potential_path):
                    mask_path = potential_path
                    break

            if mask_path and os.path.exists(img_path):
                test_data.append((img_path, mask_path))
                print(f"âœ… æ·»åŠ æµ‹è¯•æ ·æœ¬: {img_file}")
            else:
                print(f"âš ï¸  è·³è¿‡æ ·æœ¬ï¼ˆç¼ºå°‘æ ‡ç­¾ï¼‰: {img_file}")

        print(f"æœ€ç»ˆæµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_data)}")
        return test_data

    except Exception as e:
        print(f"âŒ åˆ›å»ºæµ‹è¯•æ•°æ®é›†å¤±è´¥: {e}")
        return []

def simple_preprocess(img_path, target_size=321):
    """ç®€å•çš„å›¾åƒé¢„å¤„ç†"""

    try:
        img = Image.open(img_path).convert('RGB')
        original_size = img.size
        img = img.resize((target_size, target_size), Image.Resampling.LANCZOS)

        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
        img_array = np.array(img) / 255.0
        img_tensor = torch.from_numpy(img_array).float()
        img_tensor = img_tensor.permute(2, 0, 1)  # HWC -> CHW

        # ImageNetå½’ä¸€åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        img_tensor = (img_tensor - mean) / std

        return img_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å›¾ç‰‡å¤±è´¥ {img_path}: {e}")
        return None

def visualize_simple_predictions(model, cfg, num_samples=3):
    """ç®€åŒ–çš„å¯è§†åŒ–é¢„æµ‹ç»“æœ"""

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_simple_test_dataset(cfg)

    if not test_data:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¯è§†åŒ–")
        return

    # PASCAL VOC ç±»åˆ«åç§°
    class_names = [
        'background', 'aeroplane', 'bicycle', 'bird', 'boat',
        'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
        'diningtable', 'dog', 'horse', 'motorbike', 'person',
        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
    ]

    # åˆ›å»ºé¢œè‰²æ˜ å°„
    colors = plt.cm.tab20(np.linspace(0, 1, len(class_names)))

    # è®¾ç½®å›¾ç‰‡å¸ƒå±€
    num_samples = min(num_samples, len(test_data))
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))
    if num_samples == 1:
        axes = axes.reshape(1, -1)

    print(f"å¼€å§‹é¢„æµ‹ {num_samples} å¼ å›¾ç‰‡...")

    successful_predictions = 0

    with torch.no_grad():
        for i, (img_path, mask_path) in enumerate(test_data[:num_samples]):
            try:
                print(f"\nå¤„ç†å›¾ç‰‡ {i+1}: {os.path.basename(img_path)}")

                # é¢„å¤„ç†å›¾åƒ
                img_tensor = simple_preprocess(img_path, cfg['crop_size'])
                if img_tensor is None:
                    continue

                if torch.cuda.is_available():
                    img_tensor = img_tensor.cuda()

                # æ¨¡å‹é¢„æµ‹
                try:
                    pred = model(img_tensor)
                    print(f"  é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {pred.shape}")
                    pred = torch.argmax(pred, dim=1)
                except Exception as pred_error:
                    print(f"  âŒ æ¨¡å‹é¢„æµ‹å¤±è´¥: {pred_error}")
                    continue

                # åŠ è½½åŸå§‹å›¾åƒå’Œæ ‡ç­¾
                original_img = Image.open(img_path).convert('RGB')
                original_img = original_img.resize((cfg['crop_size'], cfg['crop_size']), Image.Resampling.LANCZOS)

                # å°è¯•åŠ è½½æ ‡ç­¾å›¾åƒ
                try:
                    mask_img = Image.open(mask_path)
                    if mask_img.mode != 'L':  # å¦‚æœä¸æ˜¯ç°åº¦å›¾ï¼Œè½¬æ¢ä¸ºç°åº¦
                        mask_img = mask_img.convert('L')
                    mask_img = mask_img.resize((cfg['crop_size'], cfg['crop_size']), Image.Resampling.NEAREST)
                    mask_array = np.array(mask_img)
                except Exception as mask_error:
                    print(f"  âŒ åŠ è½½æ ‡ç­¾å¤±è´¥: {mask_error}")
                    # åˆ›å»ºè™šæ‹Ÿæ ‡ç­¾
                    mask_array = np.zeros((cfg['crop_size'], cfg['crop_size']))

                # è½¬æ¢é¢„æµ‹ç»“æœ
                pred_array = pred[0].cpu().numpy()

                print(f"  é¢„æµ‹æ•°ç»„å½¢çŠ¶: {pred_array.shape}")
                print(f"  æ ‡ç­¾æ•°ç»„å½¢çŠ¶: {mask_array.shape}")
                print(f"  é¢„æµ‹ç±»åˆ«èŒƒå›´: {pred_array.min()} - {pred_array.max()}")
                print(f"  æ ‡ç­¾ç±»åˆ«èŒƒå›´: {mask_array.min()} - {mask_array.max()}")

                # åˆ›å»ºå½©è‰²åˆ†å‰²å›¾
                def colorize_mask(mask, colors):
                    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3))
                    unique_classes = np.unique(mask)
                    for class_id in unique_classes:
                        if class_id < len(colors):
                            color_mask[mask == class_id] = colors[class_id][:3]
                    return color_mask

                colored_gt = colorize_mask(mask_array, colors)
                colored_pred = colorize_mask(pred_array, colors)

                # ç»˜åˆ¶ç»“æœ
                axes[i, 0].imshow(original_img)
                axes[i, 0].set_title(f'Original Image {i+1}')
                axes[i, 0].axis('off')

                axes[i, 1].imshow(colored_gt)
                axes[i, 1].set_title('Ground Truth')
                axes[i, 1].axis('off')

                axes[i, 2].imshow(colored_pred)
                axes[i, 2].set_title('Prediction')
                axes[i, 2].axis('off')

                successful_predictions += 1
                print(f"âœ… å›¾ç‰‡ {i+1} é¢„æµ‹å®Œæˆ")

            except Exception as e:
                print(f"âŒ å›¾ç‰‡ {i+1} é¢„æµ‹å¤±è´¥: {e}")
                # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯åœ¨å›¾ç‰‡ä¸Š
                if i < len(axes):
                    for j in range(3):
                        axes[i, j].text(0.5, 0.5, f'Error:\n{str(e)[:50]}...', 
                                       ha='center', va='center', transform=axes[i, j].transAxes,
                                       fontsize=8, bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.7))
                        axes[i, j].axis('off')
                continue

    plt.tight_layout()
    plt.show()

    print(f"\n=== é¢„æµ‹ç»“æœæ€»ç»“ ===")
    print(f"æˆåŠŸé¢„æµ‹: {successful_predictions}/{num_samples}")

    if successful_predictions > 0:
        # æ˜¾ç¤ºç±»åˆ«é¢œè‰²æ˜ å°„
        fig, ax = plt.subplots(1, 1, figsize=(12, 6))
        for i, (name, color) in enumerate(zip(class_names, colors)):
            ax.barh(i, 1, color=color[:3])
        ax.set_yticks(range(len(class_names)))
        ax.set_yticklabels(class_names)
        ax.set_xlabel('Class Color Mapping')
        ax.set_title('PASCAL VOC Class Colors')
        plt.tight_layout()
        plt.show()
    else:
        print("âŒ æ‰€æœ‰é¢„æµ‹éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å’Œæ•°æ®")

# ä¸»æ‰§è¡Œå‡½æ•°
def main():
    print("=== å¼€å§‹å¯è§†åŒ–è®­ç»ƒç»“æœ ===")

    # åŠ è½½æ¨¡å‹
    model, cfg = load_trained_model()

    if model is None:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹")
        return

    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    print("\nå¼€å§‹å¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    try:
        visualize_simple_predictions(model, cfg, num_samples=3)
        print("\nâœ… å¯è§†åŒ–å®Œæˆ")
    except Exception as e:
        print(f"\nâŒ å¯è§†åŒ–è¿‡ç¨‹å‡ºé”™: {e}")

# è¿è¡Œå¯è§†åŒ–
main()


# In[ ]:


# Cell: å¤‡ç”¨æ–¹æ¡ˆ - æ‰‹åŠ¨è§£æå’Œå¯è§†åŒ–è®­ç»ƒæ•°æ®
import os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def manual_parse_and_visualize():
    """æ‰‹åŠ¨è§£æäº‹ä»¶æ–‡ä»¶å¹¶å¯è§†åŒ–"""

    log_dir = Path(r"/kaggle/input/origin-event")
    events_files = list(log_dir.glob("events.out.tfevents.*"))

    if not events_files:
        print("âŒ æœªæ‰¾åˆ°äº‹ä»¶æ–‡ä»¶")
        return

    print(f"ğŸ“Š æ‰‹åŠ¨è§£æ {len(events_files)} ä¸ªäº‹ä»¶æ–‡ä»¶...")

    all_data = {}

    for events_file in events_files:
        print(f"\nè§£ææ–‡ä»¶: {events_file.name}")

        try:
            # å°è¯•ä½¿ç”¨ TensorFlow è§£æ
            import tensorflow as tf

            for event in tf.compat.v1.train.summary_iterator(str(events_file)):
                if event.summary:
                    for value in event.summary.value:
                        tag = value.tag
                        scalar_value = value.simple_value
                        step = event.step

                        if tag not in all_data:
                            all_data[tag] = {'steps': [], 'values': []}

                        all_data[tag]['steps'].append(step)
                        all_data[tag]['values'].append(scalar_value)

            print(f"âœ… æˆåŠŸè§£æ")

        except ImportError:
            print("âŒ TensorFlow æœªå®‰è£…ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
            continue
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            continue

    if not all_data:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ•°æ®")
        return

    # å¯è§†åŒ–æ•°æ®
    print(f"\nğŸ“ˆ æ‰¾åˆ° {len(all_data)} ä¸ªæŒ‡æ ‡:")
    for tag in all_data.keys():
        print(f"   - {tag}: {len(all_data[tag]['values'])} ä¸ªæ•°æ®ç‚¹")

    # åˆ›å»ºå›¾è¡¨
    num_metrics = len(all_data)
    cols = 2
    rows = (num_metrics + 1) // 2

    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))

    if num_metrics == 1:
        axes = [axes]
    elif num_metrics <= 2:
        axes = axes if isinstance(axes, list) else [axes]
    else:
        axes = axes.flatten()

    for i, (tag, data) in enumerate(all_data.items()):
        if i < len(axes):
            steps = np.array(data['steps'])
            values = np.array(data['values'])

            axes[i].plot(steps, values, 'b-', linewidth=2, marker='o', markersize=4)
            axes[i].set_title(f'{tag}', fontsize=12)
            axes[i].set_xlabel('Step')
            axes[i].set_ylabel('Value')
            axes[i].grid(True, alpha=0.3)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if len(values) > 0:
                final_value = values[-1]
                max_value = np.max(values)
                min_value = np.min(values)

                info_text = f'Final: {final_value:.4f}\nMax: {max_value:.4f}\nMin: {min_value:.4f}'
                axes[i].text(0.02, 0.98, info_text, 
                           transform=axes[i].transAxes, 
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8),
                           verticalalignment='top',
                           fontsize=8)

    # éšè—å¤šä½™çš„å­å›¾
    for i in range(len(all_data), len(axes)):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

    return all_data

# è¿è¡Œæ‰‹åŠ¨è§£æ
training_data = manual_parse_and_visualize()


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UniMatch è®­ç»ƒç»“æœåˆ†æè„šæœ¬
åˆ†æè®­ç»ƒæ—¥å¿—ã€æ¨¡å‹æ€§èƒ½å’Œå¯è§†åŒ–ç»“æœ
"""

import os
import re
import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import json
from datetime import datetime

def analyze_training_results(result_dir):
    """åˆ†æè®­ç»ƒç»“æœçš„ä¸»å‡½æ•°"""
    result_dir = Path(result_dir)
    print(f"ğŸ” åˆ†æè®­ç»ƒç»“æœç›®å½•: {result_dir}")
    
    # 1. åˆ†ææ¨¡å‹æ–‡ä»¶
    analyze_model_files(result_dir)
    
    # 2. åˆ†æè®­ç»ƒæ—¥å¿—
    analyze_training_logs(result_dir)
    
    # 3. åˆ†æäº‹ä»¶æ–‡ä»¶ï¼ˆå¦‚æœæœ‰tensorboardæ—¥å¿—ï¼‰
    analyze_tensorboard_events(result_dir)
    
    # 4. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_summary_report(result_dir)

def analyze_model_files(result_dir):
    """åˆ†ææ¨¡å‹æ–‡ä»¶"""
    print("\nğŸ“Š æ¨¡å‹æ–‡ä»¶åˆ†æ:")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_files = {
        'best.pth': 'æœ€ä½³æ¨¡å‹',
        'latest.pth': 'æœ€æ–°æ¨¡å‹'
    }
    
    for filename, description in model_files.items():
        filepath = result_dir / filename
        if filepath.exists():
            try:
                checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
                print(f"âœ… {description} ({filename}):")
                print(f"   - æ–‡ä»¶å¤§å°: {filepath.stat().st_size / (1024*1024):.1f} MB")
                
                if 'epoch' in checkpoint:
                    print(f"   - è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
                if 'best_miou' in checkpoint:
                    print(f"   - æœ€ä½³mIoU: {checkpoint['best_miou']:.4f}")
                if 'model' in checkpoint:
                    model_size = sum(p.numel() for p in checkpoint['model'].values())
                    print(f"   - æ¨¡å‹å‚æ•°é‡: {model_size/1e6:.1f}M")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†æ•°é˜¶èåˆå‚æ•°
                frac_params = [k for k in checkpoint['model'].keys() if 'frac_up' in k]
                if frac_params:
                    print(f"   - åˆ†æ•°é˜¶èåˆå‚æ•°: {len(frac_params)} ä¸ª")
                    print(f"     åŒ…å«: {', '.join(frac_params[:3])}{'...' if len(frac_params) > 3 else ''}")
                else:
                    print("   - æœªå‘ç°åˆ†æ•°é˜¶èåˆå‚æ•°")
                    
            except Exception as e:
                print(f"âŒ æ— æ³•åŠ è½½ {filename}: {e}")
        else:
            print(f"âŒ æœªæ‰¾åˆ° {description} ({filename})")

def analyze_training_logs(result_dir):
    """åˆ†æè®­ç»ƒæ—¥å¿—"""
    print("\nğŸ“ˆ è®­ç»ƒæ—¥å¿—åˆ†æ:")
    
    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    log_files = list(result_dir.glob("*.log")) + list(result_dir.glob("train.out"))
    
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        return
    
    for log_file in log_files:
        print(f"\nåˆ†ææ—¥å¿—æ–‡ä»¶: {log_file.name}")
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–è®­ç»ƒä¿¡æ¯
            epochs, losses, mious = extract_training_metrics(content)
            
            if epochs:
                print(f"âœ… è®­ç»ƒè½®æ•°: {len(epochs)} è½®")
                print(f"âœ… æœ€ç»ˆloss: {losses[-1]:.4f}")
                if mious:
                    print(f"âœ… æœ€ä½³mIoU: {max(mious):.4f}")
                
                # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
                plot_training_curves(epochs, losses, mious, result_dir)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæŒ‡æ ‡")
                
        except Exception as e:
            print(f"âŒ åˆ†ææ—¥å¿—å¤±è´¥: {e}")

def extract_training_metrics(log_content):
    """ä»æ—¥å¿—ä¸­æå–è®­ç»ƒæŒ‡æ ‡"""
    epochs = []
    losses = []
    mious = []
    
    # åŒ¹é…è®­ç»ƒlossçš„æ¨¡å¼
    loss_pattern = r'Epoch:\s*(\d+).*?Total loss:\s*([\d.]+)'
    loss_matches = re.findall(loss_pattern, log_content)
    
    for epoch, loss in loss_matches:
        epochs.append(int(epoch))
        losses.append(float(loss))
    
    # åŒ¹é…éªŒè¯mIoUçš„æ¨¡å¼
    miou_pattern = r'Epoch:\s*(\d+).*?mIoU:\s*([\d.]+)'
    miou_matches = re.findall(miou_pattern, log_content)
    
    epoch_to_miou = {int(epoch): float(miou) for epoch, miou in miou_matches}
    mious = [epoch_to_miou.get(epoch, 0) for epoch in epochs]
    
    return epochs, losses, mious

def plot_training_curves(epochs, losses, mious, result_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        plt.figure(figsize=(12, 5))
        
        # Lossæ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(epochs, losses, 'b-', linewidth=2, label='Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Loss Curve')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # mIoUæ›²çº¿
        plt.subplot(1, 2, 2)
        if any(mious):
            valid_epochs = [e for e, m in zip(epochs, mious) if m > 0]
            valid_mious = [m for m in mious if m > 0]
            if valid_epochs:
                plt.plot(valid_epochs, valid_mious, 'r-', linewidth=2, label='Validation mIoU')
                plt.xlabel('Epoch')
                plt.ylabel('mIoU')
                plt.title('Validation mIoU Curve')
                plt.grid(True, alpha=0.3)
                plt.legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        save_path = result_dir / 'training_curves.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
        
    except Exception as e:
        print(f"âš ï¸ ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")

def analyze_tensorboard_events(result_dir):
    """åˆ†æTensorBoardäº‹ä»¶æ–‡ä»¶"""
    print("\nğŸ“‹ TensorBoardäº‹ä»¶åˆ†æ:")
    
    # æŸ¥æ‰¾äº‹ä»¶æ–‡ä»¶
    event_files = list(result_dir.glob("events.out.tfevents.*"))
    
    if not event_files:
        print("âŒ æœªæ‰¾åˆ°TensorBoardäº‹ä»¶æ–‡ä»¶")
        return
    
    try:
        # éœ€è¦å®‰è£…tensorboard: pip install tensorboard
        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
        
        for event_file in event_files:
            print(f"âœ… å‘ç°äº‹ä»¶æ–‡ä»¶: {event_file.name}")
            
            ea = EventAccumulator(str(event_file))
            ea.Reload()
            
            # è·å–å¯ç”¨çš„æ ‡é‡
            scalar_tags = ea.Tags()['scalars']
            print(f"   è®°å½•çš„æŒ‡æ ‡: {', '.join(scalar_tags)}")
            
    except ImportError:
        print("âš ï¸ éœ€è¦å®‰è£…tensorboardæ¥åˆ†æäº‹ä»¶æ–‡ä»¶: pip install tensorboard")
    except Exception as e:
        print(f"âš ï¸ åˆ†æäº‹ä»¶æ–‡ä»¶å¤±è´¥: {e}")

def generate_summary_report(result_dir):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print("\nğŸ“„ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š:")
    
    report = {
        'analysis_time': datetime.now().isoformat(),
        'result_directory': str(result_dir),
        'files_found': [],
        'model_info': {},
        'training_summary': {}
    }
    
    # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
    for file in result_dir.iterdir():
        if file.is_file():
            report['files_found'].append({
                'name': file.name,
                'size_mb': file.stat().st_size / (1024*1024),
                'modified': datetime.fromtimestamp(file.stat().st_mtime).isoformat()
            })
    
    # å°è¯•åŠ è½½æœ€ä½³æ¨¡å‹ä¿¡æ¯
    best_model_path = result_dir / 'best.pth'
    if best_model_path.exists():
        try:
            checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
            report['model_info'] = {
                'epoch': checkpoint.get('epoch', 'unknown'),
                'best_miou': checkpoint.get('best_miou', 'unknown'),
                'has_fractional_fusion': any('frac_up' in k for k in checkpoint.get('model', {}).keys())
            }
        except:
            pass
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = result_dir / 'analysis_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # æ‰“å°å…³é”®ä¿¡æ¯
    print("\nğŸ¯ å…³é”®ç»“æœ:")
    if 'best_miou' in report['model_info']:
        print(f"   æœ€ä½³mIoU: {report['model_info']['best_miou']}")
    if 'epoch' in report['model_info']:
        print(f"   è®­ç»ƒè½®æ•°: {report['model_info']['epoch']}")
    if report['model_info'].get('has_fractional_fusion'):
        print("   âœ… åŒ…å«åˆ†æ•°é˜¶èåˆæ¨¡å—")
    else:
        print("   âŒ æœªåŒ…å«åˆ†æ•°é˜¶èåˆæ¨¡å—")

def compare_models(result_dirs):
    """æ¯”è¾ƒå¤šä¸ªè®­ç»ƒç»“æœ"""
    print("\nğŸ” æ¨¡å‹å¯¹æ¯”åˆ†æ:")
    
    results = []
    for result_dir in result_dirs:
        result_dir = Path(result_dir)
        if not result_dir.exists():
            continue
            
        best_model_path = result_dir / 'best.pth'
        if not best_model_path.exists():
            continue
            
        try:
            checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
            results.append({
                'name': result_dir.name,
                'path': str(result_dir),
                'miou': checkpoint.get('best_miou', 0),
                'epoch': checkpoint.get('epoch', 0),
                'has_frac': any('frac_up' in k for k in checkpoint.get('model', {}).keys())
            })
        except:
            continue
    
    if results:
        # æŒ‰mIoUæ’åº
        results.sort(key=lambda x: x['miou'], reverse=True)
        
        print("æ’å  | æ¨¡å‹åç§° | mIoU   | è½®æ•° | åˆ†æ•°é˜¶èåˆ")
        print("-" * 50)
        for i, result in enumerate(results, 1):
            frac_mark = "âœ…" if result['has_frac'] else "âŒ"
            print(f"{i:2d}    | {result['name'][:15]:<15} | {result['miou']:.4f} | {result['epoch']:3d}  | {frac_mark}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†æUniMatchè®­ç»ƒç»“æœ')
    parser.add_argument('--result-dir', type=str, 
                       default='/root/autodl-tmp/unimatch_workspace/exp/pascal/unimatch/r101/732',
                       help='è®­ç»ƒç»“æœç›®å½•')
    parser.add_argument('--compare', nargs='+', help='æ¯”è¾ƒå¤šä¸ªè®­ç»ƒç»“æœç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸ¯ UniMatch è®­ç»ƒç»“æœåˆ†æ")
    print("=" * 50)
    
    if args.compare:
        compare_models(args.compare)
    else:
        analyze_training_results(args.result_dir)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
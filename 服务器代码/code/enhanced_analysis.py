#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆè®­ç»ƒç»“æœåˆ†æè„šæœ¬
"""

import os
import re
import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import json

def enhanced_log_analysis(result_dir):
    """å¢å¼ºçš„æ—¥å¿—åˆ†æ"""
    result_dir = Path(result_dir)
    print(f"\nğŸ” è¯¦ç»†æ—¥å¿—åˆ†æ: {result_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ—¥å¿—æ–‡ä»¶
    log_patterns = ["*.log", "*.out", "train*", "*.txt"]
    log_files = []
    for pattern in log_patterns:
        log_files.extend(result_dir.glob(pattern))
    
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")
        return
    
    all_metrics = {
        'epochs': [],
        'losses': [],
        'loss_x': [],
        'loss_s': [],
        'mious': [],
        'timestamps': []
    }
    
    for log_file in log_files:
        print(f"\nğŸ“– åˆ†ææ–‡ä»¶: {log_file.name}")
        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # å¤šç§æ—¥å¿—æ ¼å¼çš„è§£æ
            parse_unimatch_logs(content, all_metrics)
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    if all_metrics['epochs']:
        create_enhanced_plots(all_metrics, result_dir)
        print_detailed_stats(all_metrics)
    else:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæŒ‡æ ‡ï¼Œå°è¯•æ‰‹åŠ¨æ£€æŸ¥æ—¥å¿—æ ¼å¼...")
        manual_log_inspection(result_dir)

def parse_unimatch_logs(content, metrics):
    """è§£æUniMatchçš„å¤šç§æ—¥å¿—æ ¼å¼"""
    
    # æ ¼å¼1: Iters: X, Total loss: Y.YYY
    pattern1 = r'Iters:\s*(\d+),\s*Total loss:\s*([\d.]+)'
    matches1 = re.findall(pattern1, content)
    for iter_num, loss in matches1:
        metrics['epochs'].append(int(iter_num))
        metrics['losses'].append(float(loss))
    
    # æ ¼å¼2: Loss x: X.XXX, Loss s: Y.YYY
    pattern2 = r'Loss x:\s*([\d.]+),\s*Loss s:\s*([\d.]+)'
    matches2 = re.findall(pattern2, content)
    for loss_x, loss_s in matches2:
        metrics['loss_x'].append(float(loss_x))
        metrics['loss_s'].append(float(loss_s))
    
    # æ ¼å¼3: Epoch: X, ... mIoU: Y.YYYY
    pattern3 = r'Epoch:\s*(\d+).*?mIoU:\s*([\d.]+)'
    matches3 = re.findall(pattern3, content, re.DOTALL)
    epoch_miou = {}
    for epoch, miou in matches3:
        epoch_miou[int(epoch)] = float(miou)
    
    # å¯¹é½mIoUåˆ°epoch
    for epoch in metrics['epochs']:
        if epoch in epoch_miou:
            metrics['mious'].append(epoch_miou[epoch])
        else:
            metrics['mious'].append(0)
    
    # æ ¼å¼4: æ—¶é—´æˆ³æ ¼å¼
    timestamp_pattern = r'\[(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'
    timestamps = re.findall(timestamp_pattern, content)
    metrics['timestamps'].extend(timestamps)

def create_enhanced_plots(metrics, result_dir):
    """åˆ›å»ºå¢å¼ºçš„å¯è§†åŒ–å›¾è¡¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('UniMatch Training Analysis', fontsize=16, fontweight='bold')
    
    # 1. Lossæ›²çº¿
    if metrics['losses']:
        axes[0, 0].plot(metrics['epochs'], metrics['losses'], 'b-', linewidth=2, alpha=0.7)
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Iterations')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(metrics['losses']) > 10:
            z = np.polyfit(metrics['epochs'], metrics['losses'], 1)
            p = np.poly1d(z)
            axes[0, 0].plot(metrics['epochs'], p(metrics['epochs']), "r--", alpha=0.8, label='Trend')
            axes[0, 0].legend()
    
    # 2. Lossç»„ä»¶å¯¹æ¯”
    if metrics['loss_x'] and metrics['loss_s']:
        min_len = min(len(metrics['loss_x']), len(metrics['loss_s']), len(metrics['epochs']))
        x_range = metrics['epochs'][:min_len]
        axes[0, 1].plot(x_range, metrics['loss_x'][:min_len], 'r-', label='Loss X', linewidth=2)
        axes[0, 1].plot(x_range, metrics['loss_s'][:min_len], 'g-', label='Loss S', linewidth=2)
        axes[0, 1].set_title('Loss Components')
        axes[0, 1].set_xlabel('Iterations')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    
    # 3. mIoUæ›²çº¿
    if any(m > 0 for m in metrics['mious']):
        valid_indices = [i for i, m in enumerate(metrics['mious']) if m > 0]
        valid_epochs = [metrics['epochs'][i] for i in valid_indices]
        valid_mious = [metrics['mious'][i] for i in valid_indices]
        
        axes[1, 0].plot(valid_epochs, valid_mious, 'g-', linewidth=2, marker='o', markersize=4)
        axes[1, 0].set_title('Validation mIoU')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('mIoU')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ ‡æ³¨æœ€ä½³ç‚¹
        if valid_mious:
            best_idx = np.argmax(valid_mious)
            best_epoch = valid_epochs[best_idx]
            best_miou = valid_mious[best_idx]
            axes[1, 0].annotate(f'Best: {best_miou:.4f}', 
                              xy=(best_epoch, best_miou), 
                              xytext=(10, 10), textcoords='offset points',
                              bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                              arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
    
    # 4. è®­ç»ƒç»Ÿè®¡
    axes[1, 1].axis('off')
    stats_text = generate_stats_text(metrics, result_dir)
    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, 
                   fontsize=10, verticalalignment='top', fontfamily='monospace',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.5))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    save_path = result_dir / 'enhanced_training_analysis.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å¢å¼ºåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")

def generate_stats_text(metrics, result_dir):
    """ç”Ÿæˆç»Ÿè®¡æ–‡æœ¬"""
    stats = []
    stats.append("ğŸ“Š è®­ç»ƒç»Ÿè®¡ä¿¡æ¯")
    stats.append("=" * 20)
    
    if metrics['losses']:
        stats.append(f"æ€»è¿­ä»£æ¬¡æ•°: {len(metrics['losses'])}")
        stats.append(f"åˆå§‹Loss: {metrics['losses'][0]:.4f}")
        stats.append(f"æœ€ç»ˆLoss: {metrics['losses'][-1]:.4f}")
        loss_reduction = (metrics['losses'][0] - metrics['losses'][-1]) / metrics['losses'][0] * 100
        stats.append(f"Lossä¸‹é™: {loss_reduction:.1f}%")
    
    if any(m > 0 for m in metrics['mious']):
        valid_mious = [m for m in metrics['mious'] if m > 0]
        stats.append(f"æœ€ä½³mIoU: {max(valid_mious):.4f}")
        stats.append(f"éªŒè¯æ¬¡æ•°: {len(valid_mious)}")
    
    # æ£€æŸ¥åˆ†æ•°é˜¶èåˆ
    best_model = result_dir / 'best.pth'
    if best_model.exists():
        try:
            checkpoint = torch.load(best_model, map_location='cpu', weights_only=False)
            frac_params = [k for k in checkpoint['model'].keys() if 'frac_up' in k]
            if frac_params:
                stats.append("âœ… åˆ†æ•°é˜¶èåˆ: å·²å¯ç”¨")
                stats.append(f"   å‚æ•°æ•°é‡: {len(frac_params)}")
            else:
                stats.append("âŒ åˆ†æ•°é˜¶èåˆ: æœªå¯ç”¨")
        except:
            pass
    
    return '\n'.join(stats)

def print_detailed_stats(metrics):
    """æ‰“å°è¯¦ç»†ç»Ÿè®¡"""
    print("\nğŸ“ˆ è¯¦ç»†è®­ç»ƒç»Ÿè®¡:")
    
    if metrics['losses']:
        print(f"   è®­ç»ƒè¿­ä»£: {len(metrics['losses'])} æ¬¡")
        print(f"   LossèŒƒå›´: {min(metrics['losses']):.4f} - {max(metrics['losses']):.4f}")
        
        # Lossè¶‹åŠ¿
        if len(metrics['losses']) > 1:
            recent_avg = np.mean(metrics['losses'][-10:])
            early_avg = np.mean(metrics['losses'][:10])
            print(f"   Lossè¶‹åŠ¿: {early_avg:.4f} â†’ {recent_avg:.4f}")
    
    if any(m > 0 for m in metrics['mious']):
        valid_mious = [m for m in metrics['mious'] if m > 0]
        print(f"   éªŒè¯æ¬¡æ•°: {len(valid_mious)}")
        print(f"   mIoUèŒƒå›´: {min(valid_mious):.4f} - {max(valid_mious):.4f}")

def manual_log_inspection(result_dir):
    """æ‰‹åŠ¨æ£€æŸ¥æ—¥å¿—æ ¼å¼"""
    print("\nğŸ” æ‰‹åŠ¨æ—¥å¿—æ£€æŸ¥:")
    
    for log_file in result_dir.glob("*"):
        if log_file.is_file() and log_file.suffix in ['.log', '.out', '.txt']:
            print(f"\nğŸ“„ {log_file.name} å†…å®¹ç¤ºä¾‹:")
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    # æ˜¾ç¤ºå‰10è¡Œå’Œå10è¡Œ
                    print("å‰10è¡Œ:")
                    for i, line in enumerate(lines[:10], 1):
                        print(f"{i:2d}: {line.strip()}")
                    
                    if len(lines) > 20:
                        print("...")
                        print("å10è¡Œ:")
                        for i, line in enumerate(lines[-10:], len(lines)-9):
                            print(f"{i:2d}: {line.strip()}")
            except Exception as e:
                print(f"è¯»å–å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    result_dir = "/root/autodl-tmp/unimatch_workspace/exp/pascal/unimatch/r101/732"
    
    print("ğŸ¯ UniMatch å¢å¼ºè®­ç»ƒç»“æœåˆ†æ")
    print("=" * 50)
    
    enhanced_log_analysis(result_dir)
    
    print("\nâœ… å¢å¼ºåˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
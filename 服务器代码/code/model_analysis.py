#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ¥ä»æ¨¡å‹æ–‡ä»¶åˆ†æè®­ç»ƒç»“æœ
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import json
import os

def direct_model_analysis():
    """ç›´æ¥ä»æ¨¡å‹æ–‡ä»¶åˆ†æè®­ç»ƒç»“æœ"""
    result_dir = Path("/root/autodl-tmp/unimatch_workspace/exp/pascal/unimatch/r101/732")
    
    print("ğŸ” ç›´æ¥æ¨¡å‹æ–‡ä»¶åˆ†æ")
    print("=" * 50)
    
    # 1. åˆ†ææ¨¡å‹æ–‡ä»¶
    analyze_model_checkpoints(result_dir)
    
    # 2. æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
    find_all_files(result_dir)
    
    # 3. æ£€æŸ¥ä¸Šçº§ç›®å½•çš„æ—¥å¿—
    check_parent_logs()
    
    # 4. ç”ŸæˆåŸºäºæ¨¡å‹çš„å¯è§†åŒ–
    create_model_based_visualization(result_dir)

def analyze_model_checkpoints(result_dir):
    """åˆ†ææ¨¡å‹checkpoint"""
    print("\nğŸ“Š æ¨¡å‹æ–‡ä»¶è¯¦ç»†åˆ†æ:")
    
    model_files = ['best.pth', 'latest.pth']
    
    for filename in model_files:
        filepath = result_dir / filename
        if not filepath.exists():
            print(f"âŒ {filename} ä¸å­˜åœ¨")
            continue
            
        try:
            print(f"\nğŸ” åˆ†æ {filename}:")
            checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
            
            # åŸºæœ¬ä¿¡æ¯
            print(f"   æ–‡ä»¶å¤§å°: {filepath.stat().st_size / (1024*1024):.2f} MB")
            
            # æ£€æŸ¥æ‰€æœ‰é”®
            print(f"   CheckpointåŒ…å«çš„é”®: {list(checkpoint.keys())}")
            
            # è¯¦ç»†åˆ†ææ¯ä¸ªé”®
            for key, value in checkpoint.items():
                if key == 'model':
                    model_params = list(value.keys())
                    print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {len(model_params)}")
                    
                    # åˆ†æ•°é˜¶èåˆå‚æ•°
                    frac_params = [k for k in model_params if 'frac' in k.lower()]
                    if frac_params:
                        print(f"   âœ… åˆ†æ•°é˜¶èåˆå‚æ•° ({len(frac_params)}ä¸ª):")
                        for param in frac_params[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                            print(f"      - {param}")
                        if len(frac_params) > 5:
                            print(f"      - ... è¿˜æœ‰ {len(frac_params)-5} ä¸ª")
                    else:
                        print(f"   âŒ æœªæ‰¾åˆ°åˆ†æ•°é˜¶èåˆå‚æ•°")
                    
                    # backboneå‚æ•°
                    backbone_params = [k for k in model_params if 'backbone' in k]
                    print(f"   backboneå‚æ•°: {len(backbone_params)} ä¸ª")
                    
                    # åˆ†ç±»å™¨å‚æ•°
                    classifier_params = [k for k in model_params if 'classifier' in k]
                    print(f"   åˆ†ç±»å™¨å‚æ•°: {len(classifier_params)} ä¸ª")
                    
                elif key == 'epoch':
                    print(f"   è®­ç»ƒè½®æ•°: {value}")
                elif key == 'best_miou':
                    if isinstance(value, (int, float)):
                        print(f"   æœ€ä½³mIoU: {value:.4f}")
                    else:
                        print(f"   æœ€ä½³mIoU: {value}")
                elif key == 'optimizer':
                    print(f"   ä¼˜åŒ–å™¨çŠ¶æ€: å·²ä¿å­˜")
                else:
                    print(f"   {key}: {type(value).__name__}")
                    
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")

def find_all_files(result_dir):
    """æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ–‡ä»¶"""
    print(f"\nğŸ“ ç›®å½•æ–‡ä»¶åˆ—è¡¨ ({result_dir}):")
    
    if not result_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {result_dir}")
        return
    
    all_files = []
    for item in result_dir.iterdir():
        if item.is_file():
            size_mb = item.stat().st_size / (1024*1024)
            all_files.append((item.name, size_mb, item.suffix))
    
    # æŒ‰å¤§å°æ’åº
    all_files.sort(key=lambda x: x[1], reverse=True)
    
    print("   æ–‡ä»¶å                          å¤§å°(MB)   ç±»å‹")
    print("   " + "-" * 50)
    for name, size, ext in all_files:
        print(f"   {name:<30} {size:>8.2f}   {ext}")

def check_parent_logs():
    """æ£€æŸ¥ä¸Šçº§ç›®å½•çš„æ—¥å¿—æ–‡ä»¶"""
    print(f"\nğŸ” æœç´¢è®­ç»ƒæ—¥å¿—:")
    
    # æœç´¢å¯èƒ½çš„æ—¥å¿—ä½ç½®
    search_paths = [
        "/root/autodl-tmp/",
        "/root/autodl-tmp/unimatch_workspace/",
        "/root/autodl-tmp/unimatch_workspace/UniMatch/",
    ]
    
    log_patterns = ["*.log", "*.out", "train*", "nohup*", "*.txt"]
    
    found_logs = []
    for search_path in search_paths:
        search_dir = Path(search_path)
        if search_dir.exists():
            for pattern in log_patterns:
                found_logs.extend(search_dir.glob(pattern))
    
    if found_logs:
        print("   æ‰¾åˆ°çš„æ—¥å¿—æ–‡ä»¶:")
        for log_file in found_logs:
            size_mb = log_file.stat().st_size / (1024*1024)
            print(f"   ğŸ“„ {log_file} ({size_mb:.2f} MB)")
            
            # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹ç¤ºä¾‹
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    if lines:
                        print(f"      é¦–è¡Œ: {lines[0].strip()}")
                        if len(lines) > 1:
                            print(f"      æœ«è¡Œ: {lines[-1].strip()}")
            except:
                pass
    else:
        print("   âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")

def create_model_based_visualization(result_dir):
    """åŸºäºæ¨¡å‹ä¿¡æ¯åˆ›å»ºå¯è§†åŒ–"""
    print(f"\nğŸ“Š åˆ›å»ºæ¨¡å‹åˆ†æå›¾è¡¨:")
    
    # å°è¯•ä»ä¸¤ä¸ªæ¨¡å‹æ–‡ä»¶æå–ä¿¡æ¯
    model_info = {}
    
    for filename in ['best.pth', 'latest.pth']:
        filepath = result_dir / filename
        if filepath.exists():
            try:
                checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
                model_info[filename] = {
                    'epoch': checkpoint.get('epoch', 'unknown'),
                    'miou': checkpoint.get('best_miou', 'unknown'),
                    'file_size': filepath.stat().st_size / (1024*1024),
                    'has_frac': any('frac' in k.lower() for k in checkpoint.get('model', {}).keys())
                }
            except:
                model_info[filename] = None
    
    if not any(model_info.values()):
        print("   âŒ æ— æ³•è¯»å–æ¨¡å‹ä¿¡æ¯")
        return
    
    # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('UniMatch Model Analysis', fontsize=16, fontweight='bold')
    
    # 1. æ¨¡å‹å¯¹æ¯”
    models = []
    epochs = []
    mious = []
    sizes = []
    
    for filename, info in model_info.items():
        if info:
            models.append(filename.replace('.pth', ''))
            epochs.append(info['epoch'] if isinstance(info['epoch'], int) else 0)
            mious.append(info['miou'] if isinstance(info['miou'], (int, float)) else 0)
            sizes.append(info['file_size'])
    
    if models:
        # è½®æ•°å¯¹æ¯”
        ax1.bar(models, epochs, color=['skyblue', 'lightcoral'][:len(models)])
        ax1.set_title('Training Epochs')
        ax1.set_ylabel('Epochs')
        
        # mIoUå¯¹æ¯”
        if any(mious):
            ax2.bar(models, mious, color=['lightgreen', 'orange'][:len(models)])
            ax2.set_title('Best mIoU')
            ax2.set_ylabel('mIoU')
            ax2.set_ylim(0, 1)
        
        # æ–‡ä»¶å¤§å°å¯¹æ¯”
        ax3.bar(models, sizes, color=['gold', 'plum'][:len(models)])
        ax3.set_title('Model Size (MB)')
        ax3.set_ylabel('Size (MB)')
    
    # 4. åˆ†æ•°é˜¶èåˆçŠ¶æ€
    ax4.axis('off')
    summary_text = "ğŸ¯ è®­ç»ƒæ€»ç»“\n" + "="*20 + "\n"
    
    for filename, info in model_info.items():
        if info:
            summary_text += f"\nğŸ“„ {filename}:\n"
            summary_text += f"   è½®æ•°: {info['epoch']}\n"
            summary_text += f"   mIoU: {info['miou']}\n"
            summary_text += f"   å¤§å°: {info['file_size']:.1f}MB\n"
            summary_text += f"   åˆ†æ•°é˜¶èåˆ: {'âœ…' if info['has_frac'] else 'âŒ'}\n"
    
    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, 
             fontsize=10, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    save_path = result_dir / 'model_analysis.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ… æ¨¡å‹åˆ†æå›¾è¡¨å·²ä¿å­˜: {save_path}")

def extract_metrics_from_tensorboard():
    """ä»TensorBoardäº‹ä»¶æ–‡ä»¶æå–æŒ‡æ ‡"""
    print(f"\nğŸ“ˆ å°è¯•ä»TensorBoardæå–æŒ‡æ ‡:")
    
    event_files = list(Path("/root/autodl-tmp/unimatch_workspace/exp/pascal/unimatch/r101/732").glob("events.out.*"))
    
    if not event_files:
        print("   âŒ æœªæ‰¾åˆ°TensorBoardäº‹ä»¶æ–‡ä»¶")
        return
    
    try:
        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
        
        for event_file in event_files:
            print(f"   ğŸ“Š åˆ†æäº‹ä»¶æ–‡ä»¶: {event_file.name}")
            
            ea = EventAccumulator(str(event_file))
            ea.Reload()
            
            # è·å–æ ‡é‡æ•°æ®
            scalar_tags = ea.Tags()['scalars']
            print(f"   å¯ç”¨æŒ‡æ ‡: {scalar_tags}")
            
            # æå–å¹¶å¯è§†åŒ–æŒ‡æ ‡
            if scalar_tags:
                fig, axes = plt.subplots(1, len(scalar_tags), figsize=(15, 5))
                if len(scalar_tags) == 1:
                    axes = [axes]
                
                for i, tag in enumerate(scalar_tags):
                    scalar_events = ea.Scalars(tag)
                    steps = [event.step for event in scalar_events]
                    values = [event.value for event in scalar_events]
                    
                    axes[i].plot(steps, values, linewidth=2)
                    axes[i].set_title(tag)
                    axes[i].set_xlabel('Step')
                    axes[i].set_ylabel('Value')
                    axes[i].grid(True, alpha=0.3)
                
                plt.tight_layout()
                save_path = Path("/root/autodl-tmp/unimatch_workspace/exp/pascal/unimatch/r101/732") / 'tensorboard_metrics.png'
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"   âœ… TensorBoardæŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜: {save_path}")
            
    except ImportError:
        print("   âš ï¸ éœ€è¦å®‰è£…tensorboard: pip install tensorboard")
    except Exception as e:
        print(f"   âŒ æå–å¤±è´¥: {e}")

if __name__ == "__main__":
    direct_model_analysis()
    extract_metrics_from_tensorboard()
    print("\nâœ… ç›´æ¥æ¨¡å‹åˆ†æå®Œæˆï¼")